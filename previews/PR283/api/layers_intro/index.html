<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>An introduction to the Layers module in Metalhead.jl · Metalhead.jl</title><meta name="title" content="An introduction to the Layers module in Metalhead.jl · Metalhead.jl"/><meta property="og:title" content="An introduction to the Layers module in Metalhead.jl · Metalhead.jl"/><meta property="twitter:title" content="An introduction to the Layers module in Metalhead.jl · Metalhead.jl"/><meta name="description" content="Documentation for Metalhead.jl."/><meta property="og:description" content="Documentation for Metalhead.jl."/><meta property="twitter:description" content="Documentation for Metalhead.jl."/><meta property="og:url" content="https://fluxml.ai/Metalhead.jl/stable/api/layers_intro/"/><meta property="twitter:url" content="https://fluxml.ai/Metalhead.jl/stable/api/layers_intro/"/><link rel="canonical" href="https://fluxml.ai/Metalhead.jl/stable/api/layers_intro/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Metalhead.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/quickstart/">A guide to getting started with Metalhead</a></li><li><a class="tocitem" href="../../tutorials/pretrained/">Working with pre-trained models from Metalhead</a></li></ul></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../../howto/resnet/">Using the ResNet model family in Metalhead.jl</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Metalhead</a></li><li><span class="tocitem">API reference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Convolutional Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../resnet/">ResNet-like models</a></li><li><a class="tocitem" href="../densenet/">DenseNet</a></li><li><a class="tocitem" href="../efficientnet/">EfficientNet family of models</a></li><li><a class="tocitem" href="../mobilenet/">MobileNet family of models</a></li><li><a class="tocitem" href="../inception/">Inception family of models</a></li><li><a class="tocitem" href="../hybrid/">Hybrid CNN architectures</a></li><li><a class="tocitem" href="../others/">Other models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox"/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Mixers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mixers/">MLPMixer-like models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Vision Transformers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../vit/">Vision Transformer models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-4" type="checkbox" checked/><label class="tocitem" for="menuitem-5-4"><span class="docs-label">Layers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>An introduction to the <code>Layers</code> module in Metalhead.jl</a><ul class="internal"><li><a class="tocitem" href="#Convolution-Normalisation:-the-conv_norm-layer"><span>Convolution + Normalisation: the <code>conv_norm</code> layer</span></a></li><li><a class="tocitem" href="#Normalisation-layers"><span>Normalisation layers</span></a></li><li><a class="tocitem" href="#Dropout-layers"><span>Dropout layers</span></a></li><li><a class="tocitem" href="#Pooling-layers"><span>Pooling layers</span></a></li><li><a class="tocitem" href="#Classifier-creation"><span>Classifier creation</span></a></li></ul></li><li><a class="tocitem" href="../layers_adv/">More advanced layers</a></li></ul></li><li><a class="tocitem" href="../utilities/">Model Utilities</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API reference</a></li><li><a class="is-disabled">Layers</a></li><li class="is-active"><a href>An introduction to the <code>Layers</code> module in Metalhead.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>An introduction to the <code>Layers</code> module in Metalhead.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Metalhead.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Metalhead.jl/blob/master/docs/src/api/layers_intro.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="layers-intro"><a class="docs-heading-anchor" href="#layers-intro">An introduction to the <code>Layers</code> module in Metalhead.jl</a><a id="layers-intro-1"></a><a class="docs-heading-anchor-permalink" href="#layers-intro" title="Permalink"></a></h1><p>Since v0.8, Metalhead.jl exports a <code>Layers</code> module that contains a number of useful layers and utilities for building neural networks. This guide will walk you through the most commonly used layers and utilities present in the <code>Layers</code> module, and how to use them. It also contains some examples of how these layers are used in Metalhead.jl as well as a comprehensive API reference.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The <code>Layers</code> module is still a work in progress. While we will endeavour to keep the API stable, we cannot guarantee that it will not change in the future. In particular, the API may change significantly between major versions of Metalhead.jl. If you find any of the functions in this module do not work as expected, please open an issue on GitHub.</p></div></div><p>First, however, you want to make sure that the <code>Layers</code> module is loaded, and that the functions and types are available in your current scope. You can do this by running the following code:</p><pre><code class="language-julia hljs">using Metalhead
using Metalhead.Layers</code></pre><h2 id="Convolution-Normalisation:-the-conv_norm-layer"><a class="docs-heading-anchor" href="#Convolution-Normalisation:-the-conv_norm-layer">Convolution + Normalisation: the <code>conv_norm</code> layer</a><a id="Convolution-Normalisation:-the-conv_norm-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution-Normalisation:-the-conv_norm-layer" title="Permalink"></a></h2><p>One of the most common patterns in modern neural networks is to have a convolutional layer followed by a normalisation layer. Most major deep learning libraries have a way to combine these two layers into a single layer. In Metalhead.jl, this is done with the <a href="#Metalhead.Layers.conv_norm"><code>Metalhead.Layers.conv_norm</code></a> layer. The function signature for this is given below:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.conv_norm" href="#Metalhead.Layers.conv_norm"><code>Metalhead.Layers.conv_norm</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,
          activation = relu; norm_layer = BatchNorm, revnorm::Bool = false,
          preact::Bool = false, stride::Integer = 1, pad::Integer = 0,
          dilation::Integer = 1, groups::Integer = 1, [bias, weight, init])</code></pre><p>Create a convolution + normalisation layer pair with activation.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>norm_layer</code>: the normalisation layer used. Note that using <code>identity</code> as the normalisation layer will result in no normalisation being applied. (This is only compatible with <code>preact</code> and <code>revnorm</code> both set to <code>false</code>.)</li><li><code>revnorm</code>: set to <code>true</code> to place the normalisation layer before the convolution</li><li><code>preact</code>: set to <code>true</code> to place the activation function before the normalisation layer (only compatible with <code>revnorm = false</code>)</li><li><code>bias</code>: bias for the convolution kernel. This is set to <code>false</code> by default if <code>norm_layer</code> is not <code>identity</code> and <code>true</code> otherwise.</li><li><code>stride</code>: stride of the convolution kernel</li><li><code>pad</code>: padding of the convolution kernel</li><li><code>dilation</code>: dilation of the convolution kernel</li><li><code>groups</code>: groups for the convolution kernel</li><li><code>weight</code>, <code>init</code>: initialization for the convolution kernel (see <code>Flux.Conv</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/conv.jl#L1-L28">source</a></section></article><p>To know more about the exact details of each of these parameters, you can refer to the documentation for this function. For now, we will focus on some common use cases. For example, if you want to create a convolutional layer with a kernel size of 3x3, with 32 input channels and 64 output channels, along with a <code>BatchNorm</code> layer, you can do the following:</p><pre><code class="language-julia hljs">conv_norm((3, 3), 32, 64)</code></pre><p>This returns a <code>Vector</code> with the desired layers. To use it in a model, the user should splat it into a Chain. For example:</p><pre><code class="language-julia hljs">Chain(Dense(3, 32), conv_norm((3, 3), 32, 64)..., Dense(64, 10))</code></pre><p>The default activation function for <code>conv_norm</code> is <code>relu</code>, and the default normalisation layer is <code>BatchNorm</code>. To use a different activation function, you can just pass it in as a positional argument. For example, to use a <code>sigmoid</code> activation function:</p><pre><code class="language-julia hljs">conv_norm((3, 3), 32, 64, sigmoid)</code></pre><p>Let&#39;s try something else. Suppose you want to use a <code>GroupNorm</code> layer instead of a <code>BatchNorm</code> layer. Note that <code>norm_layer</code> is a keyword argument in the function signature of <code>conv_norm</code> as shown above. Then we can write:</p><pre><code class="language-julia hljs">conv_norm((3, 3), 32, 64; norm_layer = GroupNorm)</code></pre><p>What if you want to change certain specific parameters of the <code>norm_layer</code>? For example, what if you want to change the number of groups in the <code>GroupNorm</code> layer?</p><pre><code class="language-julia hljs"># defining the norm layer
norm_layer = planes -&gt; GroupNorm(planes, 4)
# passing it to the conv_norm layer
conv_norm((3, 3), 32, 64; norm_layer = norm_layer)</code></pre><p>One of Julia&#39;s features is that functions are first-class objects, and can be passed around as arguments to other functions. Here, we have create an <a href="https://docs.julialang.org/en/v1/manual/functions/#man-anonymous-functions-1"><strong>anonymous function</strong></a> that takes in the number of planes as an argument, and returns a <code>GroupNorm</code> layer with 4 groups. This is then passed to the <code>norm_layer</code> keyword argument of the <code>conv_norm</code> layer. Using anonymous functions allows us to configure the layers in a very flexible manner, and this is a common pattern in Metalhead.jl.</p><p>Let&#39;s take a slightly more complicated example. TensorFlow uses different defaults for its normalisation layers. In particular, it uses an <code>epsilon</code> value of <code>1e-3</code> for <code>BatchNorm</code> layers. If you want to use the same defaults as TensorFlow, you can do the following:</p><pre><code class="language-julia hljs"># note that 1e-3 is not a Float32 and Flux is optimized for Float32, so we use 1.0f-3
conv_norm((3, 3), 32, 64; norm_layer = planes -&gt; BatchNorm(planes, eps = 1.0f-3))</code></pre><p>which, incidentally, is very similar to the code Metalhead uses internally for the <a href="#Metalhead.Layers.basic_conv_bn"><code>Metalhead.Layers.basic_conv_bn</code></a> layer that is used in the Inception family of models.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.basic_conv_bn" href="#Metalhead.Layers.basic_conv_bn"><code>Metalhead.Layers.basic_conv_bn</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">basic_conv_bn(kernel_size::Dims{2}, inplanes, outplanes, activation = relu;
              kwargs...)</code></pre><p>Returns a convolution + batch normalisation pair with activation as used by the Inception family of models with default values matching those used in the official TensorFlow implementation.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>batchnorm</code>: set to <code>true</code> to include batch normalization after each convolution</li><li><code>kwargs</code>: keyword arguments passed to <a href="#Metalhead.Layers.conv_norm"><code>conv_norm</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/conv.jl#L64-L80">source</a></section></article><h2 id="Normalisation-layers"><a class="docs-heading-anchor" href="#Normalisation-layers">Normalisation layers</a><a id="Normalisation-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Normalisation-layers" title="Permalink"></a></h2><p>The <code>Layers</code> module provides some custom normalisation functions that are not present in Flux.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.LayerScale" href="#Metalhead.Layers.LayerScale"><code>Metalhead.Layers.LayerScale</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LayerScale(planes::Integer, λ)</code></pre><p>Creates a <code>Flux.Scale</code> layer that performs &quot;<code>LayerScale</code>&quot; (<a href="https://arxiv.org/abs/2103.17239">reference</a>).</p><p><strong>Arguments</strong></p><ul><li><code>planes</code>: Size of channel dimension in the input.</li><li><code>λ</code>: initialisation value for the learnable diagonal matrix.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/scale.jl#L11-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.LayerNormV2" href="#Metalhead.Layers.LayerNormV2"><code>Metalhead.Layers.LayerNormV2</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LayerNormV2(size..., λ=identity; affine=true, eps=1f-5)</code></pre><p>Same as Flux&#39;s LayerNorm but eps is added before taking the square root in the denominator. Therefore, LayerNormV2 matches pytorch&#39;s LayerNorm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/normalise.jl#L38-L43">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.ChannelLayerNorm" href="#Metalhead.Layers.ChannelLayerNorm"><code>Metalhead.Layers.ChannelLayerNorm</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ChannelLayerNorm(sz::Integer, λ = identity; eps = 1.0f-6)</code></pre><p>A variant of LayerNorm where the input is normalised along the channel dimension. The input is expected to have channel dimension with size <code>sz</code>. It also applies a learnable shift and rescaling after the normalization.</p><p>Note that this is specifically for inputs with 4 dimensions in the format (H, W, C, N) where H, W are the height and width of the input, C is the number of channels, and N is the batch size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/normalise.jl#L14-L24">source</a></section></article><p>There is also a utility function, <code>prenorm</code>, which applies a normalisation layer before a given block and simply returns a <code>Chain</code> with the normalisation layer and the block. This is useful for creating Vision Transformers (ViT)-like models.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.prenorm" href="#Metalhead.Layers.prenorm"><code>Metalhead.Layers.prenorm</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">prenorm(planes, block; norm_layer = LayerNorm)</code></pre><p>Utility function to apply a normalization layer before a block.</p><p><strong>Arguments</strong></p><ul><li><code>planes</code>: Size of dimension to normalize.</li><li><code>block</code>: The block before which the normalization layer is applied.</li><li><code>norm_layer</code>: The normalization layer to use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/normalise.jl#L1-L11">source</a></section></article><h2 id="Dropout-layers"><a class="docs-heading-anchor" href="#Dropout-layers">Dropout layers</a><a id="Dropout-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Dropout-layers" title="Permalink"></a></h2><p>The <code>Layers</code> module provides two dropout-like layers not present in Flux:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.DropBlock" href="#Metalhead.Layers.DropBlock"><code>Metalhead.Layers.DropBlock</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DropBlock(drop_block_prob = 0.1, block_size = 7, gamma_scale = 1.0, [rng])</code></pre><p>The <code>DropBlock</code> layer. While training, it zeroes out continguous regions of size <code>block_size</code> in the input. During inference, it simply returns the input <code>x</code>. It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the <code>DropBlock</code> layer is equivalent to <code>identity</code>.</p><p>(<a href="https://arxiv.org/abs/1810.12890">reference</a>)</p><p><strong>Arguments</strong></p><ul><li><code>drop_block_prob</code>: probability of dropping a block. If <code>nothing</code> is passed, it returns <code>identity</code>. Note that some literature uses the term &quot;survival probability&quot; instead, which is equivalent to <code>1 - drop_block_prob</code>.</li><li><code>block_size</code>: size of the block to drop</li><li><code>gamma_scale</code>: multiplicative factor for <code>gamma</code> used. For the calculation of gamma, refer to <a href="https://arxiv.org/abs/1810.12890">the paper</a>.</li><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/drop.jl#L49-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.StochasticDepth" href="#Metalhead.Layers.StochasticDepth"><code>Metalhead.Layers.StochasticDepth</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StochasticDepth(p, mode = :row; [rng])</code></pre><p>Implements Stochastic Depth. This is a <code>Dropout</code> layer from Flux that drops values with probability <code>p</code>. (<a href="https://arxiv.org/abs/1603.09382">reference</a>)</p><p>This layer can be used to drop certain blocks in a residual structure and allow them to propagate completely through the skip connection. It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the <code>StochasticDepth</code> layer is equivalent to <code>identity</code>.</p><p><strong>Arguments</strong></p><ul><li><code>p</code>: probability of Stochastic Depth. Note that some literature uses the term &quot;survival probability&quot; instead, which is equivalent to <code>1 - p</code>.</li><li><code>mode</code>: Either <code>:batch</code> or <code>:row</code>. <code>:batch</code> randomly zeroes the entire input, <code>row</code> zeroes randomly selected rows from the batch. The default is <code>:row</code>.</li><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. See <code>Flux.Dropout</code> for more information on the behaviour of this argument. Custom RNGs are only supported on the CPU.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/drop.jl#L115-L137">source</a></section></article><p><code>DropBlock</code> also has a functional variant present in the <code>Layers</code> module:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.dropblock" href="#Metalhead.Layers.dropblock"><code>Metalhead.Layers.dropblock</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dropblock([rng], x::AbstractArray{T, 4}, drop_block_prob, block_size,
          gamma_scale, active::Bool = true)</code></pre><p>The dropblock function. If <code>active</code> is <code>true</code>, for each input, it zeroes out continguous regions of size <code>block_size</code> in the input. Otherwise, it simply returns the input <code>x</code>.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU.</li><li><code>x</code>: input array</li><li><code>drop_block_prob</code>: probability of dropping a block. If <code>nothing</code> is passed, it returns <code>identity</code>.</li><li><code>block_size</code>: size of the block to drop</li><li><code>gamma_scale</code>: multiplicative factor for <code>gamma</code> used. For the calculations, refer to <a href="https://arxiv.org/abs/1810.12890">the paper</a>.</li></ul><p>If you are not a package developer, you most likely do not want this function. Use <a href="#Metalhead.Layers.DropBlock"><code>DropBlock</code></a> instead.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/drop.jl#L13-L33">source</a></section></article><p>Both <code>DropBlock</code> and <code>StochasticDepth</code> are used along with probability values that vary based on a linear schedule across the structure of the model (see the respective papers for more details). The <code>Layers</code> module provides a utility function to create such a schedule as well:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.linear_scheduler" href="#Metalhead.Layers.linear_scheduler"><code>Metalhead.Layers.linear_scheduler</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">linear_scheduler(drop_prob = 0.0; start_value = 0.0, depth)
linear_scheduler(drop_prob::Nothing; depth::Integer)</code></pre><p>Returns the dropout probabilities for a given depth using the linear scaling rule. Note that this returns evenly spaced values between <code>start_value</code> and <code>drop_prob</code>, not including <code>drop_prob</code>. If <code>drop_prob</code> is <code>nothing</code>, it returns a <code>Vector</code> of length <code>depth</code> with all values equal to <code>nothing</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/utilities.jl#L51-L59">source</a></section></article><p>The <a href="../resnet/#Metalhead.resnet"><code>Metalhead.resnet</code></a> function which powers the ResNet family of models in Metalhead.jl is configured to allow the use of both these layers. For examples, check out the guide for using the ResNet family in Metalhead <a href="../../howto/resnet/#resnet-guide">here</a>. These layers can also be used by the user to construct other custom models.</p><h2 id="Pooling-layers"><a class="docs-heading-anchor" href="#Pooling-layers">Pooling layers</a><a id="Pooling-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling-layers" title="Permalink"></a></h2><p>The <code>Layers</code> module provides a <a href="#Metalhead.Layers.AdaptiveMeanMaxPool"><code>Metalhead.Layers.AdaptiveMeanMaxPool</code></a> layer, which is inspired by a similar layer present in <a href="https://github.com/huggingface/pytorch-image-models/blob/394e8145551191ae60f672556936314a20232a35/timm/layers/adaptive_avgmax_pool.py#L106">timm</a>. </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.AdaptiveMeanMaxPool" href="#Metalhead.Layers.AdaptiveMeanMaxPool"><code>Metalhead.Layers.AdaptiveMeanMaxPool</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdaptiveMeanMaxPool([connection = +], output_size::Tuple = (1, 1))</code></pre><p>A type of adaptive pooling layer which uses both mean and max pooling and combines them to produce a single output. Note that this is equivalent to <code>Parallel(connection, AdaptiveMeanPool(output_size), AdaptiveMaxPool(output_size))</code>. When <code>connection</code> is not specified, it defaults to <code>+</code>.</p><p><strong>Arguments</strong></p><ul><li><code>connection</code>: The connection type to use.</li><li><code>output_size</code>: The size of the output after pooling.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/pool.jl#L1-L13">source</a></section></article><p>Many mid-level model functions in Metalhead.jl have been written to support passing custom pooling layers to them if applicable (either in the model itself or in the classifier head). For example, the <a href="../resnet/#Metalhead.resnet"><code>Metalhead.resnet</code></a> function supports this, and examples of this can be found in the guide for using the ResNet family in Metalhead <a href="../../howto/resnet/#resnet-guide">here</a>.</p><h2 id="Classifier-creation"><a class="docs-heading-anchor" href="#Classifier-creation">Classifier creation</a><a id="Classifier-creation-1"></a><a class="docs-heading-anchor-permalink" href="#Classifier-creation" title="Permalink"></a></h2><p>Metalhead provides a function to create a classifier for neural network models that is quite flexible, and is used by the library extensively to create the classifier &quot;head&quot; for networks. This function is called <a href="#Metalhead.Layers.create_classifier"><code>Metalhead.Layers.create_classifier</code></a> and is documented below:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Metalhead.Layers.create_classifier" href="#Metalhead.Layers.create_classifier"><code>Metalhead.Layers.create_classifier</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">create_classifier(inplanes::Integer, nclasses::Integer, activation = identity;
                  use_conv::Bool = false, pool_layer = AdaptiveMeanPool((1, 1)), 
                  dropout_prob = nothing)</code></pre><p>Creates a classifier head to be used for models.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: number of input feature maps</li><li><code>nclasses</code>: number of output classes</li><li><code>activation</code>: activation function to use</li><li><code>use_conv</code>: whether to use a 1x1 convolutional layer instead of a <code>Dense</code> layer.</li><li><code>pool_layer</code>: pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as <code>AdaptiveMeanPool((1, 1))</code>, for example.</li><li><code>dropout_prob</code>: dropout probability used in the classifier head. Set to <code>nothing</code> to disable dropout.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/classifier.jl#L1-L17">source</a></section><section><div><pre><code class="language-julia hljs">create_classifier(inplanes::Integer, hidden_planes::Integer, nclasses::Integer,
                  activations::NTuple{2} = (relu, identity);
                  use_conv::NTuple{2, Bool} = (false, false),
                  pool_layer = AdaptiveMeanPool((1, 1)), dropout_prob = nothing)</code></pre><p>Creates a classifier head to be used for models with an extra hidden layer.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: number of input feature maps</li><li><code>hidden_planes</code>: number of hidden feature maps</li><li><code>nclasses</code>: number of output classes</li><li><code>activations</code>: activation functions to use for the hidden and output layers. This is a tuple of two elements, the first being the activation function for the hidden layer and the second for the output layer.</li><li><code>use_conv</code>: whether to use a 1x1 convolutional layer instead of a <code>Dense</code> layer. This is a tuple of two booleans, the first for the hidden layer and the second for the output layer.</li><li><code>pool_layer</code>: pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as <code>AdaptiveMeanPool((1, 1))</code>, for example.</li><li><code>dropout_prob</code>: dropout probability used in the classifier head. Set to <code>nothing</code> to disable dropout.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/05f75d7b0e1db7cb97f3fd2fd2c08280ca4e6a8c/src/layers/classifier.jl#L44-L66">source</a></section></article><p>Due to the power of multiple dispatch in Julia, the above function can be called with two different signatures - one of which creates a classifier with no hidden layers, and the other which creates a classifier with a single hidden layer. The function signature for both is documented above, and the user can choose the one that is most convenient for them. Both are used in Metalhead.jl - the latter is used in MobileNetv3, and the former is used almost everywhere else.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../vit/">« Vision Transformer models</a><a class="docs-footer-nextpage" href="../layers_adv/">More advanced layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 5 December 2024 10:18">Thursday 5 December 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
