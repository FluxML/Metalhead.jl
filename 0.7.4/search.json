[{"body":"Arguments inchannels : The number of channels in the input .  The default value is 3 . dropout : rate of dropout in classifier head . nclasses : the number of output classes .","id":"docstrings/Metalhead.inceptionresnetv2.html#arguments"},{"body":"Arguments ifilters : number of input feature maps ofilters : number of output feature maps depth : number of convolution/convolution + batch norm layers batchnorm : set to  true  to include batch normalization after each convolution","id":"docstrings/Metalhead.vgg_block.html#arguments"},{"body":"private   reluadd   —   function Convenience function for  (x, y) -> @. relu(x) + relu(y) . Useful as the  connection  argument for  resnet . See also  addrelu .","id":"docstrings/Metalhead.Layers.reluadd.html"},{"body":"private   dense_block   —   function Create a sequence of DenseNet bottlenecks increasing the number of output feature maps by  growth_rates  with each block ( reference ) .","id":"docstrings/Metalhead.dense_block.html"},{"body":"private   loadpretrain!   —   function Load the pre - trained weight artifacts matching  <name>.bson  into  model .","id":"docstrings/Metalhead.loadpretrain!.html"},{"body":"Arguments inplanes : the number of input feature maps out_1x1 : the number of output feature maps for the 1x1 convolution (branch 1) red_3x3 : the number of output feature maps for the 3x3 reduction convolution (branch 2) out_3x3 : the number of output feature maps for the 3x3 convolution (branch 2) red_5x5 : the number of output feature maps for the 5x5 reduction convolution (branch 3) out_5x5 : the number of output feature maps for the 5x5 convolution (branch 3) pool_proj : the number of output feature maps for the pooling projection (branch 4)","id":"docstrings/Metalhead._inceptionblock.html#arguments"},{"body":"Arguments: planes : number of input channels . drop_path_rate : Stochastic depth rate . λ : Initial value for  LayerScale","id":"docstrings/Metalhead.convnextblock.html#arguments"},{"body":"private   cat_channels   —   function Concatenate  x  and  y  (and any  z s) along the channel dimension (third dimension) . Equivalent to  cat(x, y, zs...; dims=3) . Convenient reduction operator for use with  Parallel .","id":"docstrings/Metalhead.Layers.cat_channels.html"},{"body":"private   reluadd   —   function Convenience function for  (x, y) -> @. relu(x) + relu(y) . Useful as the  connection  argument for  resnet . See also  addrelu .","id":"docstrings/Metalhead.reluadd.html"},{"body":"Name Module Visibility Category  AlexNet   Metalhead   public   struct   ConvMixer   Metalhead   public   struct   ConvNeXt   Metalhead   public   struct   DenseNet   Metalhead   public   struct   EfficientNet   Metalhead   public   struct   GoogLeNet   Metalhead   public   struct   InceptionResNetv2   Metalhead   public   struct   Inceptionv3   Metalhead   public   struct   Inceptionv4   Metalhead   public   struct   ChannelLayerNorm   Metalhead.Layers   public   parametric type   ClassTokens   Metalhead.Layers   public   parametric type   DropPath   Metalhead.Layers   public   function   LayerScale   Metalhead.Layers   public   function   MHAttention   Metalhead.Layers   public   parametric type   PatchEmbedding   Metalhead.Layers   public   function   ViPosEmbedding   Metalhead.Layers   public   parametric type   addrelu   Metalhead.Layers   private   function   cat_channels   Metalhead.Layers   private   function   conv_bn   Metalhead.Layers   public   function   depthwise_sep_conv_bn   Metalhead.Layers   public   function   gated_mlp_block   Metalhead.Layers   public   function   inputscale   Metalhead.Layers   private   function   invertedresidual   Metalhead.Layers   public   function   mlp_block   Metalhead.Layers   public   function   reluadd   Metalhead.Layers   private   function   skip_identity   Metalhead.Layers   public   function   skip_projection   Metalhead.Layers   public   function   squeeze_excite   Metalhead.Layers   public   function   swapdims   Metalhead.Layers   private   function   MLPMixer   Metalhead   public   struct   MobileNetv1   Metalhead   public   struct   MobileNetv2   Metalhead   public   struct   MobileNetv3   Metalhead   public   struct   ResMLP   Metalhead   public   struct   ResNeXt   Metalhead   public   struct   ResNet   Metalhead   public   struct   SpatialGatingUnit   Metalhead   private   parametric type   SqueezeNet   Metalhead   public   struct   VGG   Metalhead   public   struct   ViT   Metalhead   public   struct   Xception   Metalhead   public   struct   _inceptionblock   Metalhead   private   function   addrelu   Metalhead   private   function   alexnet   Metalhead   private   function   basicblock   Metalhead   private   function   bottleneck   Metalhead   private   function   bottleneck_v1   Metalhead   private   function   cat_channels   Metalhead   private   function   convmixer   Metalhead   private   function   convnext   Metalhead   private   function   convnextblock   Metalhead   private   function   dense_block   Metalhead   private   function   dense_bottleneck   Metalhead   private   function   densenet   Metalhead   private   function   efficientnet   Metalhead   private   function   fire   Metalhead   private   function   gMLP   Metalhead   public   struct   googlenet   Metalhead   private   function   inceptionresnetv2   Metalhead   private   function   inceptionv3   Metalhead   private   function   inceptionv3_a   Metalhead   private   function   inceptionv3_b   Metalhead   private   function   inceptionv3_c   Metalhead   private   function   inceptionv3_d   Metalhead   private   function   inceptionv3_e   Metalhead   private   function   inceptionv4   Metalhead   private   function   inputscale   Metalhead   private   function   loadpretrain!   Metalhead   private   function   mixerblock   Metalhead   private   function   mlpmixer   Metalhead   private   function   mobilenetv1   Metalhead   private   function   mobilenetv2   Metalhead   private   function   mobilenetv3   Metalhead   private   function   reluadd   Metalhead   private   function   resmixerblock   Metalhead   private   function   resnet   Metalhead   private   function   resnext   Metalhead   private   function   resnextblock   Metalhead   private   function   spatial_gating_block   Metalhead   private   function   squeezenet   Metalhead   private   function   swapdims   Metalhead   private   function   transformer_encoder   Metalhead   private   function   transition   Metalhead   private   function   vgg   Metalhead   private   function   vgg_block   Metalhead   private   function   vgg_classifier_layers   Metalhead   private   function   vgg_convolutional_layers   Metalhead   private   function   vit   Metalhead   private   function   weights   Metalhead   private   function   xception   Metalhead   private   function   xception_block   Metalhead   private   function ","id":"docstrings.html#docstring-index"},{"body":"Arguments pretrain : set to  true  to load the pre - trained weights for ImageNet inchannels : The number of channels in the input .  The default value is 3 . dropout : rate of dropout in classifier head . nclasses : the number of output classes . Inceptionv4  does not currently support pretrained weights .","id":"docstrings/Metalhead.Inceptionv4.html#arguments"},{"body":"private   inputscale   —   function Scale the input by a scalar  λ  and applies an activation function to it . Equivalent to  activation.(λ .* x) .","id":"docstrings/Metalhead.inputscale.html"},{"body":"Arguments: planes : number of input channels nheads : number of heads qkv_bias : whether to use bias in the layer to get the query, key and value attn_drop : dropout rate after the self - attention layer proj_drop : dropout rate after the projection layer","id":"docstrings/Metalhead.Layers.MHAttention.html#arguments-1"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : the number of output feature maps downsample : set to  true  to downsample the input","id":"docstrings/Metalhead.Layers.skip_projection.html#arguments"},{"body":"private   inputscale   —   function Scale the input by a scalar  λ  and applies an activation function to it . Equivalent to  activation.(λ .* x) .","id":"docstrings/Metalhead.Layers.inputscale.html"},{"body":"public   skip_identity   —   function Create a identity projection ( reference ) .","id":"docstrings/Metalhead.Layers.skip_identity.html"},{"body":"public   gated_mlp_block   —   function Feedforward block based on the implementation in the paper  “ Pay Attention to MLPs ” . ( reference )","id":"docstrings/Metalhead.Layers.gated_mlp_block.html"},{"body":"public   squeeze_excite   —   function Squeeze and excitation layer used by MobileNet variants ( reference ) .","id":"docstrings/Metalhead.Layers.squeeze_excite.html"},{"body":"Arguments inplanes : number of input feature maps to the full sequence growth_rates : the growth (additive) rates of output feature maps after each block (a vector of  k s from the ref)","id":"docstrings/Metalhead.dense_block.html#arguments"},{"body":"Arguments imsize : tuple  (width, height, channels)  indicating the size after the convolution layers (see  Metalhead.vgg_convolutional_layers ) nclasses : number of output classes fcsize : input and output size of the intermediate fully connected layer dropout : the dropout level between each fully connected layer","id":"docstrings/Metalhead.vgg_classifier_layers.html#arguments"},{"body":"Arguments gate_layer : Layer to use for the gating . inplanes : Number of dimensions in the input . hidden_planes : Number of dimensions in the intermediate layer . outplanes : Number of dimensions in the output  -  by default it is the same as  inplanes . dropout : Dropout rate . activation : Activation function to use .","id":"docstrings/Metalhead.Layers.gated_mlp_block.html#arguments"},{"body":"public   ViT   —   struct Creates a Vision Transformer (ViT) model . ( reference ) .","id":"docstrings/Metalhead.ViT.html"},{"body":"public   PatchEmbedding   —   function Patch embedding layer used by many vision transformer - like models to split the input image into patches .","id":"docstrings/Metalhead.Layers.PatchEmbedding.html"},{"body":"private   cat_channels   —   function Concatenate  x  and  y  (and any  z s) along the channel dimension (third dimension) . Equivalent to  cat(x, y, zs...; dims=3) . Convenient reduction operator for use with  Parallel .","id":"docstrings/Metalhead.cat_channels.html"},{"body":"public   MobileNetv1   —   struct Create a MobileNetv1 model with the baseline configuration ( reference ) . Set  pretrain  to  true  to load the pretrained weights for ImageNet .","id":"docstrings/Metalhead.MobileNetv1.html"},{"body":"Arguments inplanes : number of input feature maps outplanes : number of output feature maps","id":"docstrings/Metalhead.transition.html#arguments"},{"body":"private   densenet   —   function Create a DenseNet model ( reference ) .","id":"docstrings/Metalhead.densenet.html"},{"body":"private   transformer_encoder   —   function transformer _ encoder(planes, depth, nheads; mlp _ ratio = 4 . 0, dropout = 0 . ) Transformer as used in the base ViT architecture . ( reference ) .","id":"docstrings/Metalhead.transformer_encoder.html"},{"body":"private   resmixerblock   —   function Creates a block for the ResMixer architecture . ( reference ) .","id":"docstrings/Metalhead.resmixerblock.html"},{"body":"Available models Model Name Function Pre - trained?  VGG   VGG  Y (w/o BN)  ResNet   ResNet  Y  GoogLeNet   GoogLeNet  N  Inception - v3   Inceptionv3  N  Inception - v4   Inceptionv4  N  InceptionResNet - v2   Inceptionv3  N  SqueezeNet   SqueezeNet  N  DenseNet   DenseNet  N  ResNeXt   ResNeXt  N  MobileNetv1   MobileNetv1  N  MobileNetv2   MobileNetv2  N  MobileNetv3   MobileNetv3  N  EfficientNet   EfficientNet  N  MLPMixer   MLPMixer  N  ResMLP   ResMLP  N  gMLP   gMLP  N  ViT   ViT  N  ConvNeXt   ConvNeXt  N  ConvMixer   ConvMixer  N To contribute new models, see our  contributing docs .","id":"README.html#available-models"},{"body":"Arguments width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper; this is usually a value between 0 . 1 and 1 . 4) inchannels : The number of input channels .  The default value is 3 . pretrain : Whether to load the pre - trained weights for ImageNet nclasses : The number of output classes See also  Metalhead.mobilenetv2 .","id":"docstrings/Metalhead.MobileNetv2.html#arguments"},{"body":"private   efficientnet   —   function Create an EfficientNet model ( reference ) .","id":"docstrings/Metalhead.efficientnet.html"},{"body":"Arguments inplanes : number of input feature maps outplanes : number of output feature maps on bottleneck branch (and scaling factor for inner feature maps; see ref)","id":"docstrings/Metalhead.dense_bottleneck.html#arguments"},{"body":"Arguments nblocks : number of dense blocks between transitions growth_rate : the output feature map growth rate of dense blocks (i . e .   k  in the paper) reduction : the factor by which the number of feature maps is scaled across each transition nclasses : the number of output classes Create a DenseNet model with specified configuration .  Currently supported values are (121, 161, 169, 201) ( reference ) . Set  pretrain = true  to load the model with pre - trained weights for ImageNet . DenseNet  does not currently support pretrained weights . See also  Metalhead.densenet .","id":"docstrings/Metalhead.DenseNet.html#arguments"},{"body":"private   fire   —   function Create a fire module ( reference ) .","id":"docstrings/Metalhead.fire.html"},{"body":"private   xception_block   —   function Create an Xception block . ( reference )","id":"docstrings/Metalhead.xception_block.html"},{"body":"private   swapdims   —   function Convenience function for permuting the dimensions of an array . perm  is a vector or tuple specifying a permutation of the input dimensions . Equivalent to  permutedims(x, perm) .","id":"docstrings/Metalhead.Layers.swapdims.html"},{"body":"public   AlexNet   —   struct Create a  AlexNet . See also  alexnet . AlexNet  does not currently support pretrained weights .","id":"docstrings/Metalhead.AlexNet.html"},{"body":"private   vgg_convolutional_layers   —   function Create VGG convolution layers ( reference ) .","id":"docstrings/Metalhead.vgg_convolutional_layers.html"},{"body":"private   convmixer   —   function Creates a ConvMixer model . ( reference )","id":"docstrings/Metalhead.convmixer.html"},{"body":"public   ResNet   —   struct Create a  ResNet  model ( reference ) . See also  resnet .","id":"docstrings/Metalhead.ResNet.html"},{"body":"Arguments block : a function with input  (inplanes, outplanes, downsample=false)  that returns a new residual block (see  Metalhead.basicblock  and  Metalhead.bottleneck ) residuals : a 2 - tuple of functions with input  (inplanes, outplanes, downsample=false) , each of which will return a function that will be used as a new  “ skip ”  path to match a residual block . Metalhead.skip_identity  and  Metalhead.skip_projection  can be used here . connection : the binary function applied to the output of residual and skip paths in a block channel_config : the growth rate of the output feature maps within a residual block block_config : a list of the number of residual blocks at each stage nclasses : the number of output classes Create a ResNet model ( reference ) .","id":"docstrings/Metalhead.resnet.html#arguments"},{"body":"Arguments pretrain : set to  true  to load the model with pre - trained weights for ImageNet nclasses : the number of output classes GoogLeNet  does not currently support pretrained weights . See also  googlenet .","id":"docstrings/Metalhead.GoogLeNet.html#arguments"},{"body":"public   VGG   —   struct Construct a VGG model with the specified input image size .  Typically, the image size is  (224, 224) .","id":"docstrings/Metalhead.VGG.html"},{"body":"Arguments planes : number of planes in the output of each block depth : number of layers inchannels : The number of channels in the input .  The default value is 3 . kernel_size : kernel size of the convolutional layers patch_size : size of the patches activation : activation function used after the convolutional layers nclasses : number of classes in the output","id":"docstrings/Metalhead.convmixer.html#arguments"},{"body":"private   inceptionv3_a   —   function Create an Inception - v3 style - A module (ref: Fig .  5 in  paper ) .","id":"docstrings/Metalhead.inceptionv3_a.html"},{"body":"Arguments kernelsize : size of the convolution kernel (tuple) inplanes : number of input feature maps outplanes : number of output feature maps activation : the activation function for the final layer rev : set to  true  to place the batch norm before the convolution use_bn : a tuple of two booleans to specify whether to use batch normalization for the first and second convolution stride : stride of the first convolution kernel pad : padding of the first convolution kernel dilation : dilation of the first convolution kernel bias ,  weight ,  init : initialization for the convolution kernel (see  Flux.Conv ) initβ ,  initγ : initialization for the batch norm (see  Flux.BatchNorm ) ϵ ,  momentum : batch norm parameters (see  Flux.BatchNorm )","id":"docstrings/Metalhead.Layers.depthwise_sep_conv_bn.html#arguments"},{"body":"Installation","id":"README.html#installation"},{"body":"private   googlenet   —   function Create an Inception - v1 model (commonly referred to as GoogLeNet) ( reference ) .","id":"docstrings/Metalhead.googlenet.html"},{"body":"Arguments: imsize : the size of the input image inchannels : the number of channels in the input .  The default value is 3 . patch_size : the size of the patches embedplanes : the number of channels in the embedding norm_layer : the normalization layer  -  by default the identity function but otherwise takes a single argument constructor for a normalization layer like LayerNorm or BatchNorm flatten : set true to flatten the input spatial dimensions after the embedding","id":"docstrings/Metalhead.Layers.PatchEmbedding.html#arguments"},{"body":"public   skip_projection   —   function Create a skip projection ( reference ) .","id":"docstrings/Metalhead.Layers.skip_projection.html"},{"body":"Arguments kernel_size : The kernel size of the convolutional layers inplanes : The number of input feature maps hidden_planes : The number of feature maps in the hidden layer outplanes : The number of output feature maps activation : The activation function for the first two convolution layer stride : The stride of the convolutional kernel, has to be either 1 or 2 reduction : The reduction factor for the number of hidden feature maps in a squeeze and excite layer (see  squeeze_excite ) . Must be ≥ 1 or  nothing  for no squeeze and excite layer .","id":"docstrings/Metalhead.Layers.invertedresidual.html#arguments"},{"body":"Arguments name : name of default configuration (can be  :b0 ,  :b1 ,  :b2 ,  :b3 ,  :b4 ,  :b5 ,  :b6 ,  :b7 ,  :b8 ) pretrain : set to  true  to load the pre - trained weights for ImageNet","id":"docstrings/Metalhead.EfficientNet.html#arguments-1"},{"body":"public   EfficientNet   —   struct Create an EfficientNet model ( reference ) . See also  efficientnet .","id":"docstrings/Metalhead.EfficientNet.html"},{"body":"private   vit   —   function Creates a Vision Transformer (ViT) model . ( reference ) .","id":"docstrings/Metalhead.vit.html"},{"body":"Arguments pretrain : set to  true  to load the pre - trained weights for ImageNet nclasses : the number of output classes Inceptionv3  does not currently support pretrained weights .","id":"docstrings/Metalhead.Inceptionv3.html#arguments"},{"body":"Adding models To add a new model architecture to Metalhead . jl, you can  open a PR .  Keep in mind a few guiding principles for how this package is designed: reuse layers from Flux as much as possible (e . g .  use  Parallel  before defining a  Bottleneck  struct) adhere as closely as possible to a reference such as a published paper (i . e .  the structure of your model should follow intuitively from the paper) use generic functional builders (e . g .   resnet  is the core function that builds  “ ResNet - like ”  models) use multiple dispatch to add convenience constructors that wrap your functional builder When in doubt, just open a PR !  We are more than happy to help review your code to help it align with the rest of the library .  After adding a model, you might consider adding some pre - trained weights (see below) .","id":"docs/dev-guide/contributing.html#adding-models"},{"body":"private   bottleneck_v1   —   function Create a bottleneck residual block ( reference ) .  The bottleneck is composed of 3 convolutional layers with all a stride of 1 except the first convolutional layer which has a stride of 2 .","id":"docstrings/Metalhead.bottleneck_v1.html"},{"body":"Arguments planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : ratio of the number of hidden channels in the channel mixing MLP to the number of planes in the block norm_layer : the normalisation layer to use dropout : the dropout rate to use in the MLP blocks drop_path_rate : Stochastic depth rate activation : the activation function to use in the MLP blocks","id":"docstrings/Metalhead.spatial_gating_block.html#arguments"},{"body":"Contributing to Metalhead . jl We welcome contributions from anyone to Metalhead . jl !  Thank you for taking the time to make our ecosystem better . You can contribute by fixing bugs, adding new models, or adding pre - trained weights .  If you aren ’ t ready to write some code, but you think you found a bug or have a feature request, please  post an issue . Before continuing, make sure you read the  FluxML contributing guide  for general guidelines and tips .","id":"docs/dev-guide/contributing.html#contributing-to-metalheadjl"},{"body":"private   inceptionv3_d   —   function Create an Inception - v3 style - D module (ref:  pytorch ) .","id":"docstrings/Metalhead.inceptionv3_d.html"},{"body":"Arguments inchannels : The number of channels in the input .  The default value is 3 . outchannels : number of output channels . nrepeats : number of repeats of depthwise separable convolution layers . stride : stride by which to downsample the input . start_with_relu : if true, start the block with a ReLU activation . grow_at_start : if true, increase the number of channels at the first convolution .","id":"docstrings/Metalhead.xception_block.html#arguments"},{"body":"public   ChannelLayerNorm   —   parametric type A variant of LayerNorm where the input is normalised along the channel dimension .  The input is expected to have channel dimension with size sz .  It also applies a learnable shift and rescaling after the normalization . Note that this is specifically for inputs with 4 dimensions in the format (H, W, C, N) where H, W are the height and width of the input, C is the number of channels, and N is the batch size .","id":"docstrings/Metalhead.Layers.ChannelLayerNorm.html"},{"body":"Arguments mode : the model configuration, one of [:tiny, :small, :base, :large, :huge, :giant, :gigantic] imsize : image size inchannels : number of input channels patch_size : size of the patches pool : pooling type, either :class or :mean nclasses : number of classes in the output See also  Metalhead.vit .","id":"docstrings/Metalhead.ViT.html#arguments"},{"body":"Arguments inplanes : number of input feature maps squeeze_planes : number of intermediate feature maps expand1x1_planes : number of output feature maps for the 1x1 expansion convolution expand3x3_planes : number of output feature maps for the 3x3 expansion convolution","id":"docstrings/Metalhead.fire.html#arguments"},{"body":"Arguments planes : Size of channel dimension in the input . λ : initialisation value for the learnable diagonal matrix .","id":"docstrings/Metalhead.Layers.LayerScale.html#arguments"},{"body":"Arguments inplanes : number of input feature maps","id":"docstrings/Metalhead.inceptionv3_b.html#arguments"},{"body":"private   mlpmixer   —   function Creates a model with the MLPMixer architecture . ( reference ) .","id":"docstrings/Metalhead.mlpmixer.html"},{"body":"private   mobilenetv3   —   function Create a MobileNetv3 model . ( reference ) .","id":"docstrings/Metalhead.mobilenetv3.html"},{"body":"Metalhead Dev CI Coverage Metalhead . jl  provides standard machine learning vision models for use with  Flux . jl .  The architectures in this package make use of pure Flux layers, and they represent the best - practices for creating modules like residual blocks, inception blocks, etc .  in Flux .  Metalhead also provides some building blocks for more complex models in the Layers module .","id":"README.html#metalhead"},{"body":"Arguments nclasses : the number of output classes","id":"docstrings/Metalhead.alexnet.html#arguments"},{"body":"private   inceptionresnetv2   —   function Creates an InceptionResNetv2 model . ( reference )","id":"docstrings/Metalhead.inceptionresnetv2.html"},{"body":"public   ClassTokens   —   parametric type Appends class tokens to an input with embedding dimension  dim  for use in many vision transformer models .","id":"docstrings/Metalhead.Layers.ClassTokens.html"},{"body":"Arguments size : the size of the model  -  one of  small ,  base ,  large  or  huge patch_size : the size of the patches imsize : the size of the input image drop_path_rate : Stochastic depth rate nclasses : number of output classes See also  Metalhead.mlpmixer .","id":"docstrings/Metalhead.ResMLP.html#arguments"},{"body":"public   gMLP   —   struct Creates a model with the gMLP architecture . ( reference ) .","id":"docstrings/Metalhead.gMLP.html"},{"body":"Arguments depth : depth of the ResNet model .  Options include (18, 34, 50, 101, 152) . nclasses : the number of output classes For  ResNet(18)  and  ResNet(34) , the parameter - free shortcut style (type  :A ) is used in the first block and the three other blocks use type  :B  connection (following the implementation in PyTorch) .  The published version of ResNet(18)  and  ResNet(34)  used type  :A  shortcuts for all four blocks .  The example below shows how to create a 18 or 34 - layer  ResNet  using only type  :A shortcuts: The bottleneck of the orginal ResNet model has a stride of 2 on the first convolutional layer when downsampling (instead of the second convolutional layers as in ResNet v1 . 5) .  The architecture of the orignal ResNet model can be obtained as shown below:","id":"docstrings/Metalhead.ResNet.html#arguments-1"},{"body":"private   SpatialGatingUnit   —   parametric type Creates a spatial gating unit as described in the gMLP paper . ( reference )","id":"docstrings/Metalhead.SpatialGatingUnit.html"},{"body":"Arguments nclasses : the number of output classes","id":"docstrings/Metalhead.inceptionv3.html#arguments"},{"body":"private   vgg_classifier_layers   —   function Create VGG classifier (fully connected) layers ( reference ) .","id":"docstrings/Metalhead.vgg_classifier_layers.html"},{"body":"Arguments width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper) configs : A  “ list of tuples ”  configuration for each layer that details: dw : Set true to use a depthwise separable convolution or false for regular convolution o : The number of output feature maps s : The stride of the convolutional kernel r : The number of time this configuration block is repeated activate : The activation function to use throughout the network inchannels : The number of input channels .  The default value is 3 . fcsize : The intermediate fully - connected size between the convolution and final layers nclasses : The number of output classes","id":"docstrings/Metalhead.mobilenetv1.html#arguments"},{"body":"Adding pre - trained weights To add pre - trained weights for an existing model or new model, you can  open a PR .  Below, we describe the steps you should follow to get there . All Metalhead . jl model artifacts are hosted using HuggingFace .  You can find the FluxML account  here .  This  documentation from HuggingFace  will provide you with an introduction to their ModelHub .  In short, the Model Hub is a collection of Git repositories, similar to Julia packages on GitHub .  This means you can  make a pull request to our HuggingFace repositories  to upload updated weight artifacts just like you would make a PR on GitHub to upload code . Train your model or port the weights from another framework . Save the model using  BSON . jl  with  BSON.@save \"modelname.bson\" model .  It is important that your model is saved under the key  model . Compress the saved model as a tarball using  tar -cvzf modelname.tar.gz modelname.bson . Obtain the SHAs (see the  Pkg docs ) .  Edit the  Artifacts.toml  file in the Metalhead . jl repository and add entry for your model .  You can leave the URL empty for now . Open a PR on Metalhead . jl .  Be sure to ping a maintainer (e . g .   @darsnack ) to let us know that you are adding a pre - trained weight .  We will create a model repository on HuggingFace if it does not already exist . Open a PR to the  corresponding HuggingFace repo .  Do this by going to the  “ Community ”  tab in the HuggingFace repository .  PRs and discussions are shown as the same thing in the HuggingFace web app .  You can use your local Git program to make clone the repo and make PRs if you wish .  Check out the  guide on PRs to HuggingFace  for more information . Copy the download URL for the model file that you added to HuggingFace .  Make sure to grab the URL for a specific commit and not for the  main  branch . Update your Metalhead . jl PR by adding the URL to the Artifacts . toml . If the tests pass for your weights, we will merge your PR !  Your model should pass the  acctest  function in the Metalhead . jl test suite .  If your model already exists in the repo, then these tests are already in place, and you can add your model configuration to the  PRETRAINED_MODELS  list in the  runtests.jl  file .  Please refer to the ResNet tests as an example . If you want to fix existing weights, then you can follow the same set of steps .","id":"docs/dev-guide/contributing.html#adding-pre-trained-weights"},{"body":"Arguments width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper; this is usually a value between 0 . 1 and 1 . 4) inchannels : The number of input channels .  The default value is 3 . pretrain : Whether to load the pre - trained weights for ImageNet nclasses : The number of output classes See also  Metalhead.mobilenetv1 .","id":"docstrings/Metalhead.MobileNetv1.html#arguments"},{"body":"Arguments: inchannels : The number of channels in the input .  The default value is 3 . drop_path_rate : Stochastic depth rate . λ : Init value for  LayerScale nclasses : number of output classes See also  Metalhead.convnext .","id":"docstrings/Metalhead.ConvNeXt.html#arguments"},{"body":"private   inceptionv3_e   —   function Create an Inception - v3 style - E module (ref: Fig .  7 in  paper ) .","id":"docstrings/Metalhead.inceptionv3_e.html"},{"body":"Arguments inplanes : the number of input feature maps to the first dense block growth_rates : the growth rates of output feature maps within each dense_block  (a vector of vectors) reduction : the factor by which the number of feature maps is scaled across each transition nclasses : the number of output classes Create a DenseNet model ( reference ) .","id":"docstrings/Metalhead.densenet.html#arguments"},{"body":"Arguments planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : ratio of the number of hidden channels in the channel mixing MLP to the number of planes in the block mlp_layer : the MLP block to use dropout : the dropout rate to use in the MLP blocks drop_path_rate : Stochastic depth rate activation : the activation function to use in the MLP blocks λ : initialisation constant for the LayerScale","id":"docstrings/Metalhead.resmixerblock.html#arguments"},{"body":"Arguments: nheads : Number of heads qkv_layer : layer to be used for getting the query, key and value attn_drop : dropout rate after the self - attention layer projection : projection layer to be used after self - attention Multi - head self - attention layer .","id":"docstrings/Metalhead.Layers.MHAttention.html#arguments"},{"body":"private   convnext   —   function Creates the layers for a ConvNeXt model . ( reference )","id":"docstrings/Metalhead.convnext.html"},{"body":"Arguments pretrain : set to  true  to load the pre - trained weights for ImageNet inchannels : The number of channels in the input .  The default value is 3 . dropout : rate of dropout in classifier head . nclasses : the number of output classes . InceptionResNetv2  does not currently support pretrained weights .","id":"docstrings/Metalhead.InceptionResNetv2.html#arguments"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : a list of the number of output feature maps for each convolution within the residual block downsample : set to  true  to downsample the input stride : a list of the stride of the 3 convolutional layers","id":"docstrings/Metalhead.bottleneck.html#arguments"},{"body":"Arguments width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper; this is usually a value between 0 . 1 and 1 . 4) configs : a  “ list of tuples ”  configuration for each layer that details: k::Integer   -  The size of the convolutional kernel c::Float   -  The multiplier factor for deciding the number of feature maps in the hidden layer t::Integer   -  The number of output feature maps for a given block r::Integer   -  The reduction factor ( >= 1  or  nothing  to skip) for squeeze and excite layers s::Integer   -  The stride of the convolutional kernel a   -  The activation function used in the bottleneck (typically  hardswish  or  relu ) inchannels : The number of input channels .  The default value is 3 . max_width : The maximum number of feature maps in any layer of the network nclasses : the number of output classes","id":"docstrings/Metalhead.mobilenetv3.html#arguments"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : the number of output feature maps downsample : this argument is ignored but it is needed for compatibility with  resnet .","id":"docstrings/Metalhead.Layers.skip_identity.html#arguments"},{"body":"Arguments size : the size of the model  -  one of  small ,  base ,  large  or  huge patch_size : the size of the patches imsize : the size of the input image drop_path_rate : Stochastic depth rate nclasses : number of output classes See also  Metalhead.mlpmixer .","id":"docstrings/Metalhead.gMLP.html#arguments"},{"body":"private   inceptionv4   —   function Create an Inceptionv4 model . ( reference )","id":"docstrings/Metalhead.inceptionv4.html"},{"body":"public   DropPath   —   function Implements Stochastic Depth  -  equivalent to  Dropout(p; dims = 4)  when  p  ≥ 0 . ( reference )","id":"docstrings/Metalhead.Layers.DropPath.html"},{"body":"Arguments size : the size of the model  -  one of  small ,  base ,  large  or  huge patch_size : the size of the patches imsize : the size of the input image drop_path_rate : Stochastic depth rate nclasses : number of output classes See also  Metalhead.mlpmixer .","id":"docstrings/Metalhead.MLPMixer.html#arguments"},{"body":"private   inceptionv3_b   —   function Create an Inception - v3 style - B module (ref: Fig .  10 in  paper ) .","id":"docstrings/Metalhead.inceptionv3_b.html"},{"body":"private   _inceptionblock   —   function Create an inception module for use in GoogLeNet ( reference ) .","id":"docstrings/Metalhead._inceptionblock.html"},{"body":"public   MobileNetv2   —   struct Create a MobileNetv2 model with the specified configuration . ( reference ) . Set  pretrain  to  true  to load the pretrained weights for ImageNet .","id":"docstrings/Metalhead.MobileNetv2.html"},{"body":"Keyword Arguments: config  : VGG convolutional block configuration .  It is defined as a vector of tuples  (output_channels, num_convolutions)  for each block inchannels ::Integer : number of input channels batchnorm ::Bool : set to  true  to use batch normalization after each convolution nclasses ::Integer : number of output classes fcsize : intermediate fully connected layer size (see  Metalhead.vgg_classifier_layers ) dropout : dropout level between fully connected layers Create a VGG style model with specified  depth .  Available values include (11, 13, 16, 19) . ( reference ) . See also  VGG .","id":"docstrings/Metalhead.VGG.html#keyword-arguments"},{"body":"Arguments pretrain : set to  true  to load pre - trained weights for ImageNet nclasses : the number of output classes","id":"docstrings/Metalhead.AlexNet.html#arguments"},{"body":"private   squeezenet   —   function Create a SqueezeNet ( reference ) .","id":"docstrings/Metalhead.squeezenet.html"},{"body":"public   Xception   —   struct Creates an Xception model . ( reference )","id":"docstrings/Metalhead.Xception.html"},{"body":"Arguments block : the type of mixer block to use in the model  -  architecture dependent (a constructor of the form  block(embedplanes, npatches; drop_path_rate, kwargs...) ) imsize : the size of the input image inchannels : the number of input channels norm_layer : the normalization layer to use in the model patch_size : the size of the patches embedplanes : the number of channels after the patch embedding (denotes the hidden dimension) drop_path_rate : Stochastic depth rate depth : the number of blocks in the model nclasses : number of output classes kwargs : additional arguments (if any) to pass to the mixer block .  Will use the defaults if not specified .","id":"docstrings/Metalhead.mlpmixer.html#arguments"},{"body":"Arguments mode : :small or :large for the size of the model (see paper) . width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper; this is usually a value between 0 . 1 and 1 . 4) inchannels : The number of channels in the input .  The default value is 3 . pretrain : whether to load the pre - trained weights for ImageNet nclasses : the number of output classes See also  Metalhead.mobilenetv3 .","id":"docstrings/Metalhead.MobileNetv3.html#arguments"},{"body":"Fixing bugs To fix a bug in Metalhead . jl, you can  open a PR .  It would be helpful to file an issue first so that we can confirm the bug .","id":"docs/dev-guide/contributing.html#fixing-bugs"},{"body":"private   inceptionv3   —   function Create an Inception - v3 model ( reference ) .","id":"docstrings/Metalhead.inceptionv3.html"},{"body":"Arguments inchannels : number of input channels . dropout : rate of dropout in classifier head . nclasses : the number of output classes .","id":"docstrings/Metalhead.inceptionv4.html#arguments"},{"body":"Arguments channels : the number of input/output feature maps reduction = 4 : the reduction factor for the number of hidden feature maps (must be ≥ 1)","id":"docstrings/Metalhead.Layers.squeeze_excite.html#arguments"},{"body":"private   vgg   —   function Create a VGG model ( reference ) .","id":"docstrings/Metalhead.vgg.html"},{"body":"public   Inceptionv3   —   struct Create an Inception - v3 model ( reference ) . See also  inceptionv3 .","id":"docstrings/Metalhead.Inceptionv3.html"},{"body":"public   ResNeXt   —   struct Create a ResNeXt model ( reference ) .","id":"docstrings/Metalhead.ResNeXt.html"},{"body":"public   ConvMixer   —   struct Creates a ConvMixer model . ( reference )","id":"docstrings/Metalhead.ConvMixer.html"},{"body":"private   convnextblock   —   function Creates a single block of ConvNeXt . ( reference )","id":"docstrings/Metalhead.convnextblock.html"},{"body":"public   GoogLeNet   —   struct Create an Inception - v1 model (commonly referred to as  GoogLeNet ) ( reference ) .","id":"docstrings/Metalhead.GoogLeNet.html"},{"body":"Arguments: inchannels : number of input channels . depths : list with configuration for depth of each block planes : list with configuration for number of output channels in each block drop_path_rate : Stochastic depth rate . λ : Initial value for  LayerScale ( reference ) nclasses : number of output classes","id":"docstrings/Metalhead.convnext.html#arguments"},{"body":"Arguments inchannels : The number of channels in the input .  The default value is 3 . dropout : rate of dropout in classifier head . nclasses : the number of output classes .","id":"docstrings/Metalhead.xception.html#arguments"},{"body":"public   ConvNeXt   —   struct Creates a ConvNeXt model . ( reference )","id":"docstrings/Metalhead.ConvNeXt.html"},{"body":"private   basicblock   —   function Create a basic residual block ( reference ) .","id":"docstrings/Metalhead.basicblock.html"},{"body":"Quickstart Using a model from Metalhead is as simple as selecting a model from the table of  available models .  For example, below we use the pre - trained ResNet - 18 model . Now, we can use this model with Flux like any other model . First, let ’ s check the accuracy on a test image from ImageNet . We ’ ll use the popular  DataAugmentation . jl  library to crop our input image, convert it to a plain array, and normalize the pixels . Below, we train it on some randomly generated data .","id":"docs/tutorials/quickstart.html#quickstart"},{"body":"private   addrelu   —   function Convenience function for  (x, y) -> @. relu(x + y) . Useful as the  connection  argument for  resnet . See also  reluadd .","id":"docstrings/Metalhead.addrelu.html"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : a list of the number of output feature maps for each convolution within the residual block downsample : set to  true  to downsample the input","id":"docstrings/Metalhead.bottleneck_v1.html#arguments"},{"body":"Arguments pretrain : set to  true  to load the pre - trained weights for ImageNet . inchannels : The number of channels in the input .  The default value is 3 . dropout : rate of dropout in classifier head . nclasses : the number of output classes . Xception  does not currently support pretrained weights .","id":"docstrings/Metalhead.Xception.html#arguments"},{"body":"private   transition   —   function Create a DenseNet transition sequence ( reference ) .","id":"docstrings/Metalhead.transition.html"},{"body":"Arguments norm : the normalisation layer to use proj : the projection layer to use Creates a spatial gating unit as described in the gMLP paper . ( reference )","id":"docstrings/Metalhead.SpatialGatingUnit.html#arguments"},{"body":"public   mlp_block   —   function Feedforward block used in many MLPMixer - like and vision - transformer models .","id":"docstrings/Metalhead.Layers.mlp_block.html"},{"body":"Arguments inplanes : number of input feature maps pool_proj : the number of output feature maps for the pooling projection","id":"docstrings/Metalhead.inceptionv3_a.html#arguments"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : the number of output feature maps cardinality : the number of groups to use for the convolution width : the number of feature maps in each group in the bottleneck downsample : set to  true  to downsample the input","id":"docstrings/Metalhead.resnextblock.html#arguments"},{"body":"public   Inceptionv4   —   struct Creates an Inceptionv4 model . ( reference )","id":"docstrings/Metalhead.Inceptionv4.html"},{"body":"Arguments config : vector of tuples  (output_channels, num_convolutions) for each block (see  Metalhead.vgg_block ) batchnorm : set to  true  to include batch normalization after each convolution inchannels : number of input channels","id":"docstrings/Metalhead.vgg_convolutional_layers.html#arguments"},{"body":"Arguments inplanes : number of input feature maps","id":"docstrings/Metalhead.inceptionv3_e.html#arguments"},{"body":"Arguments: inplanes : the number of input feature maps outplanes : a list of the number of output feature maps for each convolution within the residual block downsample : set to  true  to downsample the input","id":"docstrings/Metalhead.basicblock.html#arguments"},{"body":"Arguments imsize : image size inchannels : number of input channels patch_size : size of the patches embedplanes : the number of channels after the patch embedding depth : number of blocks in the transformer nheads : number of attention heads in the transformer mlpplanes : number of hidden channels in the MLP block in the transformer dropout : dropout rate emb_dropout : dropout rate for the positional embedding layer pool : pooling type, either :class or :mean nclasses : number of classes in the output","id":"docstrings/Metalhead.vit.html#arguments"},{"body":"public   invertedresidual   —   function Create a basic inverted residual block for MobileNet variants ( reference ) .","id":"docstrings/Metalhead.Layers.invertedresidual.html"},{"body":"Arguments cardinality : the number of groups to use for the convolution width : the number of feature maps in each group in the bottleneck widen_factor : the factor by which the width of the bottleneck is increased after each stage connection : the binary function applied to the output of residual and skip paths in a block block_config : a list of the number of residual blocks at each stage nclasses : the number of output classes","id":"docstrings/Metalhead.resnext.html#arguments"},{"body":"public   depthwise_sep_conv_bn   —   function Create a depthwise separable convolution chain as used in MobileNetv1 . This is sequence of layers: a  kernelsize  depthwise convolution from  inplanes => inplanes a batch norm layer +  activation  (if  use_bn[1] == true ; otherwise  activation  is applied to the convolution output) a  kernelsize  convolution from  inplanes => outplanes a batch norm layer +  activation  (if  use_bn[2] == true ; otherwise  activation  is applied to the convolution output) See Fig .  3 in  reference .","id":"docstrings/Metalhead.Layers.depthwise_sep_conv_bn.html"},{"body":"Getting Started You can find the Metalhead . jl getting started guide  here .","id":"README.html#getting-started"},{"body":"Arguments channel_config : the growth rate of the output feature maps within a residual block block_config : a list of the number of residual blocks at each stage shortcut_config : the type of shortcut style (either  :A ,  :B , or  :C ) . shortcut_config  can also be a vector of symbols if different shortcut styles are applied to different residual blocks . block : a function with input  (inplanes, outplanes, downsample=false)  that returns a new residual block (see  Metalhead.basicblock  and  Metalhead.bottleneck ) connection : the binary function applied to the output of residual and skip paths in a block nclasses : the number of output classes Create a ResNet model with a specified depth ( reference ) following  these modification referred as ResNet v1 . 5 . See also  Metalhead.resnet .","id":"docstrings/Metalhead.ResNet.html#arguments"},{"body":"Arguments inplanes : number of input feature maps inner_planes : the number of output feature maps within each branch n : the  “ grid size ”  (kernel size) for the convolution layers","id":"docstrings/Metalhead.inceptionv3_c.html#arguments"},{"body":"Arguments block : a function with input  (inplanes, outplanes, downsample=false)  that returns a new residual block (see  Metalhead.basicblock  and  Metalhead.bottleneck ) shortcut_config : the type of shortcut style (either  :A ,  :B , or  :C ) :A : uses a  Metalhead.skip_identity  for all residual blocks :B : uses a  Metalhead.skip_projection  for the first residual block and  Metalhead.skip_identity  for the remaining residual blocks :C : uses a  Metalhead.skip_projection  for all residual blocks connection : the binary function applied to the output of residual and skip paths in a block channel_config : the growth rate of the output feature maps within a residual block block_config : a list of the number of residual blocks at each stage nclasses : the number of output classes","id":"docstrings/Metalhead.resnet.html#arguments-1"},{"body":"public   ResMLP   —   struct Creates a model with the ResMLP architecture . ( reference ) .","id":"docstrings/Metalhead.ResMLP.html"},{"body":"Arguments cardinality : the number of groups to use for the convolution width : the number of feature maps in each group in the bottleneck block_config : a list of the number of residual blocks at each stage nclasses : the number of output classes Create a ResNeXt model with specified configuration .  Currently supported values for  config  are (50, 101) . ( reference ) . Set  pretrain = true  to load the model with pre - trained weights for ImageNet . ResNeXt  does not currently support pretrained weights . See also  Metalhead.resnext .","id":"docstrings/Metalhead.ResNeXt.html#arguments"},{"body":"Arguments pretrain : set to  true  to load pre - trained model weights for ImageNet","id":"docstrings/Metalhead.VGG.html#arguments"},{"body":"public   LayerScale   —   function Creates a  Flux.Scale  layer that performs  “ LayerScale ” ( reference ) .","id":"docstrings/Metalhead.Layers.LayerScale.html"},{"body":"public   ViPosEmbedding   —   parametric type Positional embedding layer used by many vision transformer - like models .","id":"docstrings/Metalhead.Layers.ViPosEmbedding.html"},{"body":"Arguments nclasses : the number of output classes","id":"docstrings/Metalhead.googlenet.html#arguments"},{"body":"public   MobileNetv3   —   struct Create a MobileNetv3 model with the specified configuration . ( reference ) . Set  pretrain = true  to load the model with pre - trained weights for ImageNet .","id":"docstrings/Metalhead.MobileNetv3.html"},{"body":"private   spatial_gating_block   —   function Creates a feedforward block based on the gMLP model architecture described in the paper . ( reference )","id":"docstrings/Metalhead.spatial_gating_block.html"},{"body":"private   xception   —   function Creates an Xception model . ( reference )","id":"docstrings/Metalhead.xception.html"},{"body":"private   mixerblock   —   function Creates a feedforward block for the MLPMixer architecture . ( reference )","id":"docstrings/Metalhead.mixerblock.html"},{"body":"Arguments planes : number of input channels depth : number of attention blocks nheads : number of attention heads mlp_ratio : ratio of MLP layers to the number of input channels dropout : dropout rate","id":"docstrings/Metalhead.transformer_encoder.html#arguments"},{"body":"public   MHAttention   —   parametric type Multi - head self - attention layer .","id":"docstrings/Metalhead.Layers.MHAttention.html"},{"body":"Arguments: planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : number(s) that determine(s) the number of hidden channels in the token mixing MLP and/or the channel mixing MLP as a ratio to the number of planes in the block . mlp_layer : the MLP layer to use in the block dropout : the dropout rate to use in the MLP blocks drop_path_rate : Stochastic depth rate activation : the activation function to use in the MLP blocks","id":"docstrings/Metalhead.mixerblock.html#arguments"},{"body":"Arguments kernelsize : size of the convolution kernel (tuple) inplanes : number of input feature maps outplanes : number of output feature maps activation : the activation function for the final layer rev : set to  true  to place the batch norm before the convolution preact : set to  true  to place the activation function before the batch norm (only compatible with  rev = false ) use_bn : set to  false  to disable batch normalization (only compatible with  rev = false  and  preact = false ) stride : stride of the convolution kernel pad : padding of the convolution kernel dilation : dilation of the convolution kernel groups : groups for the convolution kernel bias ,  weight ,  init : initialization for the convolution kernel (see  Flux.Conv ) initβ ,  initγ : initialization for the batch norm (see  Flux.BatchNorm ) ϵ ,  momentum : batch norm parameters (see  Flux.BatchNorm )","id":"docstrings/Metalhead.Layers.conv_bn.html#arguments"},{"body":"Arguments imsize : input image width and height as a tuple config : the configuration for the convolution layers (see  Metalhead.vgg_convolutional_layers ) inchannels : number of input channels batchnorm : set to  true  to use batch normalization after each convolution nclasses : number of output classes fcsize : intermediate fully connected layer size (see  Metalhead.vgg_classifier_layers ) dropout : dropout level between fully connected layers","id":"docstrings/Metalhead.vgg.html#arguments"},{"body":"public   MLPMixer   —   struct Creates a model with the MLPMixer architecture . ( reference ) .","id":"docstrings/Metalhead.MLPMixer.html"},{"body":"Arguments planes : the number of planes in the block npatches : the number of patches of the input norm_layer : the normalisation layer to use","id":"docstrings/Metalhead.SpatialGatingUnit.html#arguments-1"},{"body":"private   mobilenetv2   —   function Create a MobileNetv2 model . ( reference ) .","id":"docstrings/Metalhead.mobilenetv2.html"},{"body":"private   bottleneck   —   function Create a bottleneck residual block ( reference ) .  The bottleneck is composed of 3 convolutional layers each with the given  stride . By default,  stride  implements  “ ResNet v1 . 5 ” which uses  stride == [1, 2, 1]  when  downsample == true . This version is standard across various ML frameworks . The original paper uses  stride == [2, 1, 1]  when  downsample == true  instead .","id":"docstrings/Metalhead.bottleneck.html"},{"body":"Arguments width_mult : Controls the number of output feature maps in each block (with 1 . 0 being the default in the paper) configs : A  “ list of tuples ”  configuration for each layer that details: t : The expansion factor that controls the number of feature maps in the bottleneck layer c : The number of output feature maps n : The number of times a block is repeated s : The stride of the convolutional kernel a : The activation function used in the bottleneck layer inchannels : The number of input channels .  The default value is 3 . max_width : The maximum number of feature maps in any layer of the network nclasses : The number of output classes","id":"docstrings/Metalhead.mobilenetv2.html#arguments"},{"body":"private   dense_bottleneck   —   function Create a Densenet bottleneck layer ( reference ) .","id":"docstrings/Metalhead.dense_bottleneck.html"},{"body":"Arguments p : rate of Stochastic Depth .","id":"docstrings/Metalhead.Layers.DropPath.html#arguments"},{"body":"Arguments nblocks : number of dense blocks between transitions growth_rate : the output feature map growth rate of dense blocks (i . e .   k  in the ref) reduction : the factor by which the number of feature maps is scaled across each transition nclasses : the number of output classes","id":"docstrings/Metalhead.densenet.html#arguments-1"},{"body":"private   addrelu   —   function Convenience function for  (x, y) -> @. relu(x + y) . Useful as the  connection  argument for  resnet . See also  reluadd .","id":"docstrings/Metalhead.Layers.addrelu.html"},{"body":"private   weights   —   function Load the pre - trained weights for  model  using the stored artifacts .","id":"docstrings/Metalhead.weights.html"},{"body":"private   mobilenetv1   —   function Create a MobileNetv1 model ( reference ) .","id":"docstrings/Metalhead.mobilenetv1.html"},{"body":"private   swapdims   —   function Convenience function for permuting the dimensions of an array . perm  is a vector or tuple specifying a permutation of the input dimensions . Equivalent to  permutedims(x, perm) .","id":"docstrings/Metalhead.swapdims.html"},{"body":"public   DenseNet   —   struct Create a DenseNet model ( reference ) . See also  densenet .","id":"docstrings/Metalhead.DenseNet.html"},{"body":"Arguments scalings : global width and depth scaling (given as a tuple) block_config : configuration for each inverted residual block, given as a vector of tuples with elements: n : number of block repetitions (will be scaled by global depth scaling) k : kernel size s : kernel stride e : expansion ratio i : block input channels (will be scaled by global width scaling) o : block output channels (will be scaled by global width scaling) inchannels : number of input channels nclasses : number of output classes max_width : maximum number of output channels before the fully connected classification blocks","id":"docstrings/Metalhead.efficientnet.html#arguments"},{"body":"Arguments inplanes : number of input feature maps","id":"docstrings/Metalhead.inceptionv3_d.html#arguments"},{"body":"private   resnext   —   function Create a ResNeXt model ( reference ) .","id":"docstrings/Metalhead.resnext.html"},{"body":"private   alexnet   —   function Create an AlexNet model ( reference ) .","id":"docstrings/Metalhead.alexnet.html"},{"body":"private   resnextblock   —   function Create a basic residual block as defined in the paper for ResNeXt ( reference ) .","id":"docstrings/Metalhead.resnextblock.html"},{"body":"Arguments scalings : global width and depth scaling (given as a tuple) block_config : configuration for each inverted residual block, given as a vector of tuples with elements: n : number of block repetitions (will be scaled by global depth scaling) k : kernel size s : kernel stride e : expansion ratio i : block input channels (will be scaled by global width scaling) o : block output channels (will be scaled by global width scaling) inchannels : number of input channels nclasses : number of output classes max_width : maximum number of output channels before the fully connected classification blocks Create an EfficientNet model ( reference ) . See also  efficientnet .","id":"docstrings/Metalhead.EfficientNet.html#arguments"},{"body":"Arguments mode : the mode of the model, either  :base ,  :small  or  :large inchannels : The number of channels in the input .  The default value is 3 . activation : activation function used after the convolutional layers nclasses : number of classes in the output","id":"docstrings/Metalhead.ConvMixer.html#arguments"},{"body":"public   InceptionResNetv2   —   struct Creates an InceptionResNetv2 model . ( reference )","id":"docstrings/Metalhead.InceptionResNetv2.html"},{"body":"public   SqueezeNet   —   struct Create a SqueezeNet ( reference ) . Set  pretrain=true  to load the model with pre - trained weights for ImageNet . SqueezeNet  does not currently support pretrained weights . See also  squeezenet .","id":"docstrings/Metalhead.SqueezeNet.html"},{"body":"Arguments inplanes : Number of dimensions in the input . hidden_planes : Number of dimensions in the intermediate layer . outplanes : Number of dimensions in the output  -  by default it is the same as  inplanes . dropout : Dropout rate . activation : Activation function to use .","id":"docstrings/Metalhead.Layers.mlp_block.html#arguments"},{"body":"private   resnet   —   function Create a ResNet model ( reference ) .","id":"docstrings/Metalhead.resnet.html"},{"body":"public   conv_bn   —   function Create a convolution + batch normalization pair with activation .","id":"docstrings/Metalhead.Layers.conv_bn.html"},{"body":"private   inceptionv3_c   —   function Create an Inception - v3 style - C module (ref: Fig .  6 in  paper ) .","id":"docstrings/Metalhead.inceptionv3_c.html"},{"body":"private   vgg_block   —   function A VGG block of convolution layers ( reference ) .","id":"docstrings/Metalhead.vgg_block.html"}]