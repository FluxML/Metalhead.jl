<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Layers · Metalhead.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://fluxml.ai/Metalhead.jl/stable/api/layers/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Metalhead.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/quickstart/">A guide to getting started with Metalhead</a></li><li><a class="tocitem" href="../../tutorials/pretrained/">Working with pre-trained models from Metalhead</a></li></ul></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../../howto/resnet/">Using the ResNet model family in Metalhead.jl</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Metalhead</a></li><li><span class="tocitem">API reference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Convolutional Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../resnet/">ResNet-like models</a></li><li><a class="tocitem" href="../densenet/">DenseNet</a></li><li><a class="tocitem" href="../efficientnet/">EfficientNet family of models</a></li><li><a class="tocitem" href="../mobilenet/">MobileNet family of models</a></li><li><a class="tocitem" href="../inception/">Inception family of models</a></li><li><a class="tocitem" href="../hybrid/">Hybrid CNN architectures</a></li><li><a class="tocitem" href="../others/">Other models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox"/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Mixers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mixers/">MLPMixer-like models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Vision Transformers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../vit/">Vision Transformer models</a></li></ul></li><li class="is-active"><a class="tocitem" href>Layers</a><ul class="internal"><li><a class="tocitem" href="#Convolution-BatchNorm-layers"><span>Convolution + BatchNorm layers</span></a></li><li><a class="tocitem" href="#Convolution-related-custom-blocks"><span>Convolution-related custom blocks</span></a></li><li><a class="tocitem" href="#Normalisation,-Dropout-and-Pooling-layers"><span>Normalisation, Dropout and Pooling layers</span></a></li><li><a class="tocitem" href="#Classifier-creation"><span>Classifier creation</span></a></li><li><a class="tocitem" href="#Vision-transformer-related-layers"><span>Vision transformer-related layers</span></a></li><li><a class="tocitem" href="#MLPMixer-related-blocks"><span>MLPMixer-related blocks</span></a></li><li><a class="tocitem" href="#Utilities-for-layers"><span>Utilities for layers</span></a></li></ul></li><li><a class="tocitem" href="../utilities/">Model Utilities</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API reference</a></li><li class="is-active"><a href>Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Layers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Metalhead.jl/blob/master/docs/src/api/layers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><p>Metalhead also defines a module called <code>Layers</code> which contains some custom layers that are used to configure the models in Metalhead. These layers are not available in Flux at present. To use the functions defined in the <code>Layers</code> module, you need to import it.</p><pre><code class="language-julia hljs">using Metalhead: Layers</code></pre><p>This page contains the API reference for the <code>Layers</code> module.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The <code>Layers</code> module is still a work in progress. While we will endeavour to keep the API stable, we cannot guarantee that it will not change in the future. If you find any of the functions in this module do not work as expected, please open an issue on GitHub.</p></div></div><h2 id="Convolution-BatchNorm-layers"><a class="docs-heading-anchor" href="#Convolution-BatchNorm-layers">Convolution + BatchNorm layers</a><a id="Convolution-BatchNorm-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution-BatchNorm-layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.conv_norm" href="#Metalhead.Layers.conv_norm"><code>Metalhead.Layers.conv_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,
          activation = relu; norm_layer = BatchNorm, revnorm::Bool = false,
          preact::Bool = false, stride::Integer = 1, pad::Integer = 0,
          dilation::Integer = 1, groups::Integer = 1, [bias, weight, init])</code></pre><p>Create a convolution + normalisation layer pair with activation.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>norm_layer</code>: the normalisation layer used. Note that using <code>identity</code> as the normalisation layer will result in no normalisation being applied. (This is only compatible with <code>preact</code> and <code>revnorm</code> both set to <code>false</code>.)</li><li><code>revnorm</code>: set to <code>true</code> to place the normalisation layer before the convolution</li><li><code>preact</code>: set to <code>true</code> to place the activation function before the normalisation layer (only compatible with <code>revnorm = false</code>)</li><li><code>bias</code>: bias for the convolution kernel. This is set to <code>false</code> by default if <code>norm_layer</code> is not <code>identity</code> and <code>true</code> otherwise.</li><li><code>stride</code>: stride of the convolution kernel</li><li><code>pad</code>: padding of the convolution kernel</li><li><code>dilation</code>: dilation of the convolution kernel</li><li><code>groups</code>: groups for the convolution kernel</li><li><code>weight</code>, <code>init</code>: initialization for the convolution kernel (see <a href="api/@ref"><code>Flux.Conv</code></a>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/conv.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.basic_conv_bn" href="#Metalhead.Layers.basic_conv_bn"><code>Metalhead.Layers.basic_conv_bn</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">basic_conv_bn(kernel_size::Dims{2}, inplanes, outplanes, activation = relu;
              kwargs...)</code></pre><p>Returns a convolution + batch normalisation pair with activation as used by the Inception family of models with default values matching those used in the official TensorFlow implementation.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>batchnorm</code>: set to <code>true</code> to include batch normalization after each convolution</li><li><code>kwargs</code>: keyword arguments passed to <a href="#Metalhead.Layers.conv_norm"><code>conv_norm</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/conv.jl#L64-L80">source</a></section></article><h2 id="Convolution-related-custom-blocks"><a class="docs-heading-anchor" href="#Convolution-related-custom-blocks">Convolution-related custom blocks</a><a id="Convolution-related-custom-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution-related-custom-blocks" title="Permalink"></a></h2><p>These blocks are designed to be used in convolutional neural networks. Most of these are used in the MobileNet and EfficientNet family of models, but they also feature in &quot;fancier&quot; versions of well known-models like ResNet (SE-ResNet).</p><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.dwsep_conv_norm" href="#Metalhead.Layers.dwsep_conv_norm"><code>Metalhead.Layers.dwsep_conv_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dwsep_conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,
                activation = relu; norm_layer = BatchNorm, stride::Integer = 1,
                bias::Bool = !(norm_layer !== identity), pad::Integer = 0, [bias, weight, init])</code></pre><p>Create a depthwise separable convolution chain as used in MobileNetv1. This is sequence of layers:</p><ul><li>a <code>kernel_size</code> depthwise convolution from <code>inplanes =&gt; inplanes</code></li><li>a (batch) normalisation layer + <code>activation</code> (if <code>norm_layer !== identity</code>; otherwise <code>activation</code> is applied to the convolution output)</li><li>a <code>kernel_size</code> convolution from <code>inplanes =&gt; outplanes</code></li><li>a (batch) normalisation layer + <code>activation</code> (if <code>norm_layer !== identity</code>; otherwise <code>activation</code> is applied to the convolution output)</li></ul><p>See Fig. 3 in <a href="https://arxiv.org/abs/1704.04861v1">reference</a>.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>norm_layer</code>: the normalisation layer used. Note that using <code>identity</code> as the normalisation layer will result in no normalisation being applied.</li><li><code>bias</code>: whether to use bias in the convolution layers.</li><li><code>stride</code>: stride of the first convolution kernel</li><li><code>pad</code>: padding of the first convolution kernel</li><li><code>weight</code>, <code>init</code>: initialization for the convolution kernel (see <a href="api/@ref"><code>Flux.Conv</code></a>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/mbconv.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.mbconv" href="#Metalhead.Layers.mbconv"><code>Metalhead.Layers.mbconv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,
       outplanes::Integer, activation = relu; stride::Integer,
       reduction::Union{Nothing, Real} = nothing,
       se_round_fn = x -&gt; round(Int, x), norm_layer = BatchNorm, kwargs...)</code></pre><p>Create a basic inverted residual block for MobileNet and Efficient variants. This is a sequence of layers:</p><ul><li><p>a 1x1 convolution from <code>inplanes =&gt; explanes</code> followed by a (batch) normalisation layer</p></li><li><p><code>activation</code> if <code>inplanes != explanes</code></p></li><li><p>a <code>kernel_size</code> depthwise separable convolution from <code>explanes =&gt; explanes</code></p></li><li><p>a (batch) normalisation layer</p></li><li><p>a squeeze-and-excitation block (if <code>reduction != nothing</code>) from <code>explanes =&gt; se_round_fn(explanes / reduction)</code> and back to <code>explanes</code></p></li><li><p>a 1x1 convolution from <code>explanes =&gt; outplanes</code></p></li><li><p>a (batch) normalisation layer + <code>activation</code></p></li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details.</p></div></div><p>First introduced in the MobileNetv2 paper. (See Fig. 3 in <a href="https://arxiv.org/abs/1801.04381v4">reference</a>.)</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: kernel size of the convolutional layers</li><li><code>inplanes</code>: number of input feature maps</li><li><code>explanes</code>: The number of expanded feature maps. This is the number of feature maps after the first 1x1 convolution.</li><li><code>outplanes</code>: The number of output feature maps</li><li><code>activation</code>: The activation function for the first two convolution layer</li><li><code>stride</code>: The stride of the convolutional kernel, has to be either 1 or 2</li><li><code>reduction</code>: The reduction factor for the number of hidden feature maps in a squeeze and excite layer (see <a href="#Metalhead.Layers.squeeze_excite"><code>squeeze_excite</code></a>)</li><li><code>se_round_fn</code>: The function to round the number of reduced feature maps in the squeeze and excite layer</li><li><code>norm_layer</code>: The normalization layer to use</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/mbconv.jl#L39-L80">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.fused_mbconv" href="#Metalhead.Layers.fused_mbconv"><code>Metalhead.Layers.fused_mbconv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fused_mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,
             outplanes::Integer, activation = relu;
             stride::Integer, norm_layer = BatchNorm)</code></pre><p>Create a fused inverted residual block.</p><p>This is a sequence of layers:</p><ul><li>a <code>kernel_size</code> depthwise separable convolution from <code>explanes =&gt; explanes</code></li><li>a (batch) normalisation layer</li><li>a 1x1 convolution from <code>explanes =&gt; outplanes</code> followed by a (batch) normalisation layer + <code>activation</code> if <code>inplanes != explanes</code></li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details.</p></div></div><p>Originally introduced by Google in <a href="https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html">EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML</a>. Later used in the EfficientNetv2 paper.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: kernel size of the convolutional layers</li><li><code>inplanes</code>: number of input feature maps</li><li><code>explanes</code>: The number of expanded feature maps</li><li><code>outplanes</code>: The number of output feature maps</li><li><code>activation</code>: The activation function for the first two convolution layer</li><li><code>stride</code>: The stride of the convolutional kernel, has to be either 1 or 2</li><li><code>norm_layer</code>: The normalization layer to use</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/mbconv.jl#L107-L138">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.squeeze_excite" href="#Metalhead.Layers.squeeze_excite"><code>Metalhead.Layers.squeeze_excite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">squeeze_excite(inplanes::Integer; reduction::Real = 16, round_fn = _round_channels, 
               norm_layer = identity, activation = relu, gate_activation = sigmoid)</code></pre><p>Creates a squeeze-and-excitation layer used in MobileNets, EfficientNets and SE-ResNets.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: The number of input feature maps</li><li><code>reduction</code>: The reduction factor for the number of hidden feature maps in the squeeze and excite layer. The number of hidden feature maps is calculated as <code>round_fn(inplanes / reduction)</code>.</li><li><code>round_fn</code>: The function to round the number of reduced feature maps.</li><li><code>activation</code>: The activation function for the first convolution layer</li><li><code>gate_activation</code>: The activation function for the gate layer</li><li><code>norm_layer</code>: The normalization layer to be used after the convolution layers</li><li><code>rd_planes</code>: The number of hidden feature maps in a squeeze and excite layer</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/selayers.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.effective_squeeze_excite" href="#Metalhead.Layers.effective_squeeze_excite"><code>Metalhead.Layers.effective_squeeze_excite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">effective_squeeze_excite(inplanes, gate_activation = sigmoid)</code></pre><p>Effective squeeze-and-excitation layer. (reference: <a href="https://arxiv.org/abs/1911.06667">CenterMask : Real-Time Anchor-Free Instance Segmentation</a>)</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: The number of input feature maps</li><li><code>gate_activation</code>: The activation function for the gate layer</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/selayers.jl#L29-L39">source</a></section></article><h2 id="Normalisation,-Dropout-and-Pooling-layers"><a class="docs-heading-anchor" href="#Normalisation,-Dropout-and-Pooling-layers">Normalisation, Dropout and Pooling layers</a><a id="Normalisation,-Dropout-and-Pooling-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Normalisation,-Dropout-and-Pooling-layers" title="Permalink"></a></h2><p>Metalhead provides various custom layers for normalisation, dropout and pooling which have been used to additionally customise various models.</p><h3 id="Normalisation-layers"><a class="docs-heading-anchor" href="#Normalisation-layers">Normalisation layers</a><a id="Normalisation-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Normalisation-layers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.ChannelLayerNorm" href="#Metalhead.Layers.ChannelLayerNorm"><code>Metalhead.Layers.ChannelLayerNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChannelLayerNorm(sz::Integer, λ = identity; eps = 1.0f-6)</code></pre><p>A variant of LayerNorm where the input is normalised along the channel dimension. The input is expected to have channel dimension with size <code>sz</code>. It also applies a learnable shift and rescaling after the normalization.</p><p>Note that this is specifically for inputs with 4 dimensions in the format (H, W, C, N) where H, W are the height and width of the input, C is the number of channels, and N is the batch size.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/normalise.jl#L4-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.LayerNormV2" href="#Metalhead.Layers.LayerNormV2"><code>Metalhead.Layers.LayerNormV2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LayerNormV2(size..., λ=identity; affine=true, eps=1f-5)</code></pre><p>Same as Flux&#39;s LayerNorm but eps is added before taking the square root in the denominator. Therefore, LayerNormV2 matches pytorch&#39;s LayerNorm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/normalise.jl#L28-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.LayerScale" href="#Metalhead.Layers.LayerScale"><code>Metalhead.Layers.LayerScale</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">LayerScale(planes::Integer, λ)</code></pre><p>Creates a <code>Flux.Scale</code> layer that performs &quot;<code>LayerScale</code>&quot; (<a href="https://arxiv.org/abs/2103.17239">reference</a>).</p><p><strong>Arguments</strong></p><ul><li><code>planes</code>: Size of channel dimension in the input.</li><li><code>λ</code>: initialisation value for the learnable diagonal matrix.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/scale.jl#L11-L21">source</a></section></article><h3 id="Dropout-layers"><a class="docs-heading-anchor" href="#Dropout-layers">Dropout layers</a><a id="Dropout-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Dropout-layers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.DropBlock" href="#Metalhead.Layers.DropBlock"><code>Metalhead.Layers.DropBlock</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DropBlock(drop_block_prob = 0.1, block_size = 7, gamma_scale = 1.0, [rng])</code></pre><p>The <code>DropBlock</code> layer. While training, it zeroes out continguous regions of size <code>block_size</code> in the input. During inference, it simply returns the input <code>x</code>. It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the <code>DropBlock</code> layer is equivalent to <code>identity</code>.</p><p>(<a href="https://arxiv.org/abs/1810.12890">reference</a>)</p><p><strong>Arguments</strong></p><ul><li><code>drop_block_prob</code>: probability of dropping a block. If <code>nothing</code> is passed, it returns <code>identity</code>. Note that some literature uses the term &quot;survival probability&quot; instead, which is equivalent to <code>1 - drop_block_prob</code>.</li><li><code>block_size</code>: size of the block to drop</li><li><code>gamma_scale</code>: multiplicative factor for <code>gamma</code> used. For the calculation of gamma, refer to <a href="https://arxiv.org/abs/1810.12890">the paper</a>.</li><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/drop.jl#L56-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.dropblock" href="#Metalhead.Layers.dropblock"><code>Metalhead.Layers.dropblock</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dropblock([rng], x::AbstractArray{T, 4}, drop_block_prob, block_size,
          gamma_scale, active::Bool = true)</code></pre><p>The dropblock function. If <code>active</code> is <code>true</code>, for each input, it zeroes out continguous regions of size <code>block_size</code> in the input. Otherwise, it simply returns the input <code>x</code>.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU.</li><li><code>x</code>: input array</li><li><code>drop_block_prob</code>: probability of dropping a block. If <code>nothing</code> is passed, it returns <code>identity</code>.</li><li><code>block_size</code>: size of the block to drop</li><li><code>gamma_scale</code>: multiplicative factor for <code>gamma</code> used. For the calculations, refer to <a href="https://arxiv.org/abs/1810.12890">the paper</a>.</li></ul><p>If you are not a package developer, you most likely do not want this function. Use <a href="#Metalhead.Layers.DropBlock"><code>DropBlock</code></a> instead.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/drop.jl#L13-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.StochasticDepth" href="#Metalhead.Layers.StochasticDepth"><code>Metalhead.Layers.StochasticDepth</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">StochasticDepth(p, mode = :row; [rng])</code></pre><p>Implements Stochastic Depth. This is a <code>Dropout</code> layer from Flux that drops values with probability <code>p</code>. (<a href="https://arxiv.org/abs/1603.09382">reference</a>)</p><p>This layer can be used to drop certain blocks in a residual structure and allow them to propagate completely through the skip connection. It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the <code>StochasticDepth</code> layer is equivalent to <code>identity</code>.</p><p><strong>Arguments</strong></p><ul><li><code>p</code>: probability of Stochastic Depth. Note that some literature uses the term &quot;survival probability&quot; instead, which is equivalent to <code>1 - p</code>.</li><li><code>mode</code>: Either <code>:batch</code> or <code>:row</code>. <code>:batch</code> randomly zeroes the entire input, <code>row</code> zeroes randomly selected rows from the batch. The default is <code>:row</code>.</li><li><code>rng</code>: can be used to pass in a custom RNG instead of the default. See <code>Flux.Dropout</code> for more information on the behaviour of this argument. Custom RNGs are only supported on the CPU.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/drop.jl#L123-L145">source</a></section></article><h3 id="Pooling-layers"><a class="docs-heading-anchor" href="#Pooling-layers">Pooling layers</a><a id="Pooling-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling-layers" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.AdaptiveMeanMaxPool" href="#Metalhead.Layers.AdaptiveMeanMaxPool"><code>Metalhead.Layers.AdaptiveMeanMaxPool</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AdaptiveMeanMaxPool([connection = +], output_size::Tuple = (1, 1))</code></pre><p>A type of adaptive pooling layer which uses both mean and max pooling and combines them to produce a single output. Note that this is equivalent to <code>Parallel(connection, AdaptiveMeanPool(output_size), AdaptiveMaxPool(output_size))</code>. When <code>connection</code> is not specified, it defaults to <code>+</code>.</p><p><strong>Arguments</strong></p><ul><li><code>connection</code>: The connection type to use.</li><li><code>output_size</code>: The size of the output after pooling.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/pool.jl#L1-L13">source</a></section></article><h2 id="Classifier-creation"><a class="docs-heading-anchor" href="#Classifier-creation">Classifier creation</a><a id="Classifier-creation-1"></a><a class="docs-heading-anchor-permalink" href="#Classifier-creation" title="Permalink"></a></h2><p>Metalhead provides a function to create a classifier for neural network models that is quite flexible, and is used by the library extensively to create the classifier &quot;head&quot; for networks.</p><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.create_classifier" href="#Metalhead.Layers.create_classifier"><code>Metalhead.Layers.create_classifier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">create_classifier(inplanes::Integer, nclasses::Integer, activation = identity;
                  use_conv::Bool = false, pool_layer = AdaptiveMeanPool((1, 1)), 
                  dropout_prob = nothing)</code></pre><p>Creates a classifier head to be used for models.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: number of input feature maps</li><li><code>nclasses</code>: number of output classes</li><li><code>activation</code>: activation function to use</li><li><code>use_conv</code>: whether to use a 1x1 convolutional layer instead of a <code>Dense</code> layer.</li><li><code>pool_layer</code>: pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as <code>AdaptiveMeanPool((1, 1))</code>, for example.</li><li><code>dropout_prob</code>: dropout probability used in the classifier head. Set to <code>nothing</code> to disable dropout.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/classifier.jl#L1-L17">source</a></section><section><div><pre><code class="nohighlight hljs">create_classifier(inplanes::Integer, hidden_planes::Integer, nclasses::Integer,
                  activations::NTuple{2} = (relu, identity);
                  use_conv::NTuple{2, Bool} = (false, false),
                  pool_layer = AdaptiveMeanPool((1, 1)), dropout_prob = nothing)</code></pre><p>Creates a classifier head to be used for models with an extra hidden layer.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: number of input feature maps</li><li><code>hidden_planes</code>: number of hidden feature maps</li><li><code>nclasses</code>: number of output classes</li><li><code>activations</code>: activation functions to use for the hidden and output layers. This is a tuple of two elements, the first being the activation function for the hidden layer and the second for the output layer.</li><li><code>use_conv</code>: whether to use a 1x1 convolutional layer instead of a <code>Dense</code> layer. This is a tuple of two booleans, the first for the hidden layer and the second for the output layer.</li><li><code>pool_layer</code>: pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as <code>AdaptiveMeanPool((1, 1))</code>, for example.</li><li><code>dropout_prob</code>: dropout probability used in the classifier head. Set to <code>nothing</code> to disable dropout.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/classifier.jl#L44-L66">source</a></section></article><h2 id="Vision-transformer-related-layers"><a class="docs-heading-anchor" href="#Vision-transformer-related-layers">Vision transformer-related layers</a><a id="Vision-transformer-related-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Vision-transformer-related-layers" title="Permalink"></a></h2><p>The <code>Layers</code> module contains specific layers that are used to build vision transformer (ViT)-inspired models:</p><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.MultiHeadSelfAttention" href="#Metalhead.Layers.MultiHeadSelfAttention"><code>Metalhead.Layers.MultiHeadSelfAttention</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultiHeadSelfAttention(planes::Integer, nheads::Integer = 8; qkv_bias::Bool = false, 
            attn_dropout_prob = 0., proj_dropout_prob = 0.)</code></pre><p>Multi-head self-attention layer.</p><p><strong>Arguments</strong></p><ul><li><code>planes</code>: number of input channels</li><li><code>nheads</code>: number of heads</li><li><code>qkv_bias</code>: whether to use bias in the layer to get the query, key and value</li><li><code>attn_dropout_prob</code>: dropout probability after the self-attention layer</li><li><code>proj_dropout_prob</code>: dropout probability after the projection layer</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/attention.jl#L1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.ClassTokens" href="#Metalhead.Layers.ClassTokens"><code>Metalhead.Layers.ClassTokens</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClassTokens(planes::Integer; init = Flux.zeros32)</code></pre><p>Appends class tokens to an input with embedding dimension <code>planes</code> for use in many vision transformer models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/embeddings.jl#L51-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.ViPosEmbedding" href="#Metalhead.Layers.ViPosEmbedding"><code>Metalhead.Layers.ViPosEmbedding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ViPosEmbedding(embedsize::Integer, npatches::Integer; 
               init = (dims::Dims{2}) -&gt; rand(Float32, dims))</code></pre><p>Positional embedding layer used by many vision transformer-like models.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/embeddings.jl#L33-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.PatchEmbedding" href="#Metalhead.Layers.PatchEmbedding"><code>Metalhead.Layers.PatchEmbedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">PatchEmbedding(imsize::Dims{2} = (224, 224); inchannels::Integer = 3,
               patch_size::Dims{2} = (16, 16), embedplanes = 768,
               norm_layer = planes -&gt; identity, flatten = true)</code></pre><p>Patch embedding layer used by many vision transformer-like models to split the input image into patches.</p><p><strong>Arguments</strong></p><ul><li><code>imsize</code>: the size of the input image</li><li><code>inchannels</code>: number of input channels</li><li><code>patch_size</code>: the size of the patches</li><li><code>embedplanes</code>: the number of channels in the embedding</li><li><code>norm_layer</code>: the normalization layer - by default the identity function but otherwise takes a single argument constructor for a normalization layer like LayerNorm or BatchNorm</li><li><code>flatten</code>: set true to flatten the input spatial dimensions after the embedding</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/embeddings.jl#L3-L20">source</a></section></article><h2 id="MLPMixer-related-blocks"><a class="docs-heading-anchor" href="#MLPMixer-related-blocks">MLPMixer-related blocks</a><a id="MLPMixer-related-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#MLPMixer-related-blocks" title="Permalink"></a></h2><p>Apart from this, the <code>Layers</code> module also contains certain blocks used in MLPMixer-style models:</p><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.gated_mlp_block" href="#Metalhead.Layers.gated_mlp_block"><code>Metalhead.Layers.gated_mlp_block</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gated_mlp(gate_layer, inplanes::Integer, hidden_planes::Integer, 
          outplanes::Integer = inplanes; dropout_prob = 0.0, activation = gelu)</code></pre><p>Feedforward block based on the implementation in the paper &quot;Pay Attention to MLPs&quot;. (<a href="https://arxiv.org/abs/2105.08050">reference</a>)</p><p><strong>Arguments</strong></p><ul><li><code>gate_layer</code>: Layer to use for the gating.</li><li><code>inplanes</code>: Number of dimensions in the input.</li><li><code>hidden_planes</code>: Number of dimensions in the intermediate layer.</li><li><code>outplanes</code>: Number of dimensions in the output - by default it is the same as <code>inplanes</code>.</li><li><code>dropout_prob</code>: Dropout probability.</li><li><code>activation</code>: Activation function to use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/mlp.jl#L22-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.mlp_block" href="#Metalhead.Layers.mlp_block"><code>Metalhead.Layers.mlp_block</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mlp_block(inplanes::Integer, hidden_planes::Integer, outplanes::Integer = inplanes; 
          dropout_prob = 0., activation = gelu)</code></pre><p>Feedforward block used in many MLPMixer-like and vision-transformer models.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: Number of dimensions in the input.</li><li><code>hidden_planes</code>: Number of dimensions in the intermediate layer.</li><li><code>outplanes</code>: Number of dimensions in the output - by default it is the same as <code>inplanes</code>.</li><li><code>dropout_prob</code>: Dropout probability.</li><li><code>activation</code>: Activation function to use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/mlp.jl#L2-L15">source</a></section></article><h2 id="Utilities-for-layers"><a class="docs-heading-anchor" href="#Utilities-for-layers">Utilities for layers</a><a id="Utilities-for-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities-for-layers" title="Permalink"></a></h2><p>These are some miscellaneous utilities present in the <code>Layers</code> module, and are used with other custom/inbuilt layers to make certain common operations in neural networks easier.</p><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.inputscale" href="#Metalhead.Layers.inputscale"><code>Metalhead.Layers.inputscale</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">inputscale(λ; activation = identity)</code></pre><p>Scale the input by a scalar <code>λ</code> and applies an activation function to it. Equivalent to <code>activation.(λ .* x)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/layers/scale.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.actadd" href="#Metalhead.Layers.actadd"><code>Metalhead.Layers.actadd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">actadd(activation = relu, xs...)</code></pre><p>Convenience function for adding input arrays after applying an activation function to them. Useful as the <code>connection</code> argument for the block function in <a href="api/@ref"><code>resnet</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L21-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.addact" href="#Metalhead.Layers.addact"><code>Metalhead.Layers.addact</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">addact(activation = relu, xs...)</code></pre><p>Convenience function for applying an activation function to the output after summing up the input arrays. Useful as the <code>connection</code> argument for the block function in <a href="api/@ref"><code>resnet</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L12-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.cat_channels" href="#Metalhead.Layers.cat_channels"><code>Metalhead.Layers.cat_channels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">cat_channels(x, y, zs...)</code></pre><p>Concatenate <code>x</code> and <code>y</code> (and any <code>z</code>s) along the channel dimension (third dimension). Equivalent to <code>cat(x, y, zs...; dims=3)</code>. Convenient reduction operator for use with <code>Parallel</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L30-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.flatten_chains" href="#Metalhead.Layers.flatten_chains"><code>Metalhead.Layers.flatten_chains</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">flatten_chains(m::Chain)
flatten_chains(m)</code></pre><p>Convenience function for traversing nested layers of a Chain object and flatten them  into a single iterator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L70-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.linear_scheduler" href="#Metalhead.Layers.linear_scheduler"><code>Metalhead.Layers.linear_scheduler</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">linear_scheduler(drop_prob = 0.0; start_value = 0.0, depth)
linear_scheduler(drop_prob::Nothing; depth::Integer)</code></pre><p>Returns the dropout probabilities for a given depth using the linear scaling rule. Note that this returns evenly spaced values between <code>start_value</code> and <code>drop_prob</code>, not including <code>drop_prob</code>. If <code>drop_prob</code> is <code>nothing</code>, it returns a <code>Vector</code> of length <code>depth</code> with all values equal to <code>nothing</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L51-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Metalhead.Layers.swapdims" href="#Metalhead.Layers.swapdims"><code>Metalhead.Layers.swapdims</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">swapdims(perm)</code></pre><p>Convenience function that returns a closure which permutes the dimensions of an array. <code>perm</code> is a vector or tuple specifying a permutation of the input dimensions. Equivalent to <code>permutedims(x, perm)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Metalhead.jl/blob/043c03022abc55c1974c485ab8f6385b27558a25/src/utilities.jl#L42-L48">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../vit/">« Vision Transformer models</a><a class="docs-footer-nextpage" href="../utilities/">Model Utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 14 July 2023 21:16">Friday 14 July 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
